{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Concat_Extrapolation_MidTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKof-utnBsjB"
      },
      "source": [
        "import argparse\r\n",
        "import gc\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import time\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import logging\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset\r\n",
        "import random\r\n",
        "from collections import defaultdict\r\n",
        "import torch\r\n",
        "import inspect\r\n",
        "from contextlib import contextmanager\r\n",
        "import subprocess\r\n",
        "\r\n",
        "FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'\r\n",
        "logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "  device = torch.device(\"cuda\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhWRnr3xByCO"
      },
      "source": [
        "# DATASET OPTIONS\r\n",
        "OBS_LEN = 8\r\n",
        "PRED_LEN = 12\r\n",
        "\r\n",
        "SINGLE_TRAIN_DATASET_PATH = \"/content/drive/My Drive/Datasets/extrapolation_test\"\r\n",
        "SINGLE_VAL_DATASET_PATH = \"/content/drive/My Drive/Datasets/zara1/val\"\r\n",
        "SINGLE_TEST_DATASET_PATH = \"/content/drive/My Drive/Datasets/extrapolation_test\"\r\n",
        "CHECKPOINT_NAME = \"/content/drive/My Drive/Datasets/Weights/ExtrapolationTest/Mid_test_reproduce.pt\"\r\n",
        "\r\n",
        "# NUMBER OF CONDITION FLAG - activate any one of the following flags\r\n",
        "SINGLE_CONDITIONAL_MODEL = True  # For single condition\r\n",
        "MULTI_CONDITIONAL_MODEL = False  # For multi condition\r\n",
        "MODEL_TYPE = 0  # 0 for prediction and 1 for simulation\r\n",
        "\r\n",
        "# MAX SPEEDS FOR ARGOVERSE AND ETH/UCY DATASETS\r\n",
        "# for argoverse\r\n",
        "AV_MAX_SPEED = 1.6\r\n",
        "OTHER_MAX_SPEED = 2.2\r\n",
        "AGENT_MAX_SPEED = 2\r\n",
        "\r\n",
        "# for eth/ucy\r\n",
        "ETH_MAX_SPEED = 2.0  #2.3\r\n",
        "HOTEL_MAX_SPEED = 2.0\r\n",
        "UNIV_MAX_SPEED = 2.0\r\n",
        "ZARA1_MAX_SPEED = 2.0\r\n",
        "ZARA2_MAX_SPEED = 2.0\r\n",
        "\r\n",
        "# PYTORCH DATA LOADER OPTIONS\r\n",
        "NUM_WORKERS = 2\r\n",
        "BATCH_MULTI_CONDITION = 32\r\n",
        "BATCH_SINGLE_CONDITION = 16\r\n",
        "BATCH_NORM = False\r\n",
        "ACTIVATION_RELU = 'relu'\r\n",
        "ACTIVATION_LEAKYRELU = 'leakyrelu'\r\n",
        "ACTIVATION_SIGMOID = 'sigmoid'\r\n",
        "\r\n",
        "# Time between consecutive frames\r\n",
        "FRAMES_PER_SECOND_SINGLE_CONDITION = 0.4\r\n",
        "FRAMES_PER_SECOND_MULTI_CONDITION = 0.1\r\n",
        "NORMALIZATION_FACTOR = 10\r\n",
        "\r\n",
        "# ENCODER DECODER HIDDEN DIMENSION OPTIONS FOR SINGLE AND MULTI CONDITION\r\n",
        "H_DIM_GENERATOR_MULTI_CONDITION = 32\r\n",
        "H_DIM_DISCRIMINATOR_MULTI_CONDITION = 64\r\n",
        "\r\n",
        "H_DIM_GENERATOR_SINGLE_CONDITION = 32\r\n",
        "H_DIM_DISCRIMINATOR_SINGLE_CONDITION = 64\r\n",
        "\r\n",
        "MLP_INPUT_DIM_MULTI_CONDITION = 4\r\n",
        "MLP_INPUT_DIM_SINGLE_CONDITION = 3\r\n",
        "\r\n",
        "# HYPER PARAMETERS OPTIONS\r\n",
        "G_LEARNING_RATE, D_LEARNING_RATE = 1e-3, 1e-3\r\n",
        "NUM_LAYERS = 1\r\n",
        "DROPOUT = 0\r\n",
        "NUM_EPOCHS_MULTI_CONDITION = 50\r\n",
        "NUM_EPOCHS_SINGLE_CONDITION = 25\r\n",
        "CHECKPOINT_EVERY = 100\r\n",
        "MLP_DIM = 64\r\n",
        "EMBEDDING_DIM = 16\r\n",
        "BOTTLENECK_DIM = 32\r\n",
        "NOISE_DIM = (8, )\r\n",
        "\r\n",
        "L2_LOSS_WEIGHT = 1\r\n",
        "\r\n",
        "NUM_ITERATIONS = 3200\r\n",
        "POOLING_TYPE = False\r\n",
        "AGGREGATION_TYPE = True\r\n",
        "ATTENTION_TYPE = False\r\n",
        "MAX_CONSIDERED_PED = 5\r\n",
        "USE_GPU = 1\r\n",
        "\r\n",
        "# SPEED CONTROL FLAGS\r\n",
        "TEST_METRIC = 1  # 0 for ground_truth speed. To simulate trajectories, change the flag to 1. This flag is used during testing and inference phase.\r\n",
        "TRAIN_METRIC = 0  # Used for training the model with the ground truth\r\n",
        "VERIFY_OUTPUT_SPEED = 1\r\n",
        "\r\n",
        "# ADD_SPEED_EVERY_FRAME, STOP_PED, CONSTANT_SPEED_FOR_ALL_PED, ADD_SPEED_PARTICULAR_FRAME - Only one flag out of the 4 can be activated at once.\r\n",
        "\r\n",
        "# Below flag is set to true if multi condition model on argoverse dataset is set to true.\r\n",
        "DIFFERENT_SPEED_MULTI_CONDITION = True\r\n",
        "AV_SPEED = 0.4\r\n",
        "AGENT_SPEED = 0.9\r\n",
        "OTHER_SPEED = 0.0\r\n",
        "\r\n",
        "CONSTANT_SPEED_MULTI_CONDITION = False  # CONSTANT_SPEED flag for multi condition\r\n",
        "CS_MULTI_CONDITION = 0.4  # Constant speed multi condition\r\n",
        "\r\n",
        "# Below flag is set to true if single condition model on eth/ucy dataset is set to true.\r\n",
        "\r\n",
        "# Change any one of the below flag to True\r\n",
        "STOP_PED_SINGLE_CONDITION = False  # Speed 0 will be imposed if the flag is set to True\r\n",
        "\r\n",
        "CONSTANT_SPEED_SINGLE_CONDITION = True\r\n",
        "CS_SINGLE_CONDITION = 0.55  # Constant speed single condition\r\n",
        "\r\n",
        "ANIMATED_VISUALIZATION_CHECK = 0\r\n",
        "\r\n",
        "G_STEPS = 1\r\n",
        "D_STEPS = 2\r\n",
        "SR_STEPS = 1\r\n",
        "BEST_K = 20\r\n",
        "PRINT_EVERY = 100\r\n",
        "NUM_SAMPLES = 20\r\n",
        "NOISE = True\r\n",
        "NUM_SAMPLE_CHECK = 100"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_hPPAMfByEA"
      },
      "source": [
        "import torch\r\n",
        "import random\r\n",
        "import os\r\n",
        "\r\n",
        "\r\n",
        "def bce_loss(input, target):\r\n",
        "    neg_abs = -input.abs()\r\n",
        "    loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\r\n",
        "    return loss.mean()\r\n",
        "\r\n",
        "\r\n",
        "def gan_g_loss(scores_fake):\r\n",
        "    y_fake = torch.ones_like(scores_fake) * random.uniform(0.7, 1.2)\r\n",
        "    return bce_loss(scores_fake, y_fake)\r\n",
        "\r\n",
        "\r\n",
        "def gan_d_loss(scores_real, scores_fake):\r\n",
        "    y_real = torch.ones_like(scores_real) * random.uniform(0.7, 1.2)\r\n",
        "    y_fake = torch.zeros_like(scores_fake) * random.uniform(0, 0.3)\r\n",
        "    loss_real = bce_loss(scores_real, y_real)\r\n",
        "    loss_fake = bce_loss(scores_fake, y_fake)\r\n",
        "    return loss_real + loss_fake\r\n",
        "\r\n",
        "\r\n",
        "def l2_loss(pred_traj, pred_traj_gt, loss_mask, random=0, mode='average', speed_reg=None):\r\n",
        "    seq_len, batch, _ = pred_traj.size()\r\n",
        "    if speed_reg != None:\r\n",
        "        loss = (pred_traj_gt.permute(1, 0, 2) - pred_traj.permute(1, 0, 2)) ** 2\r\n",
        "    else:\r\n",
        "        loss = (loss_mask.unsqueeze(dim=2) *\r\n",
        "            (pred_traj_gt.permute(1, 0, 2) - pred_traj.permute(1, 0, 2)) ** 2)\r\n",
        "    if mode == 'sum':\r\n",
        "        return torch.sum(loss)\r\n",
        "    elif mode == 'average':\r\n",
        "        return torch.sum(loss) / torch.numel(loss_mask.data)\r\n",
        "    elif mode == 'raw':\r\n",
        "        return loss.sum(dim=2).sum(dim=1)\r\n",
        "\r\n",
        "\r\n",
        "def mae_loss(pred_traj, pred_traj_gt, random=0, mode='average', speed_reg=None):\r\n",
        "    seq_len, batch, _ = pred_traj.size()\r\n",
        "    loss = torch.abs(pred_traj_gt.permute(1, 0, 2) - pred_traj.permute(1, 0, 2))\r\n",
        "    if mode == 'sum':\r\n",
        "        return torch.sum(loss)\r\n",
        "    elif mode == 'raw':\r\n",
        "        return loss.sum(dim=2).sum(dim=1)\r\n",
        "\r\n",
        "\r\n",
        "def displacement_error(pred_traj, pred_traj_gt, consider_ped=None, mode='sum'):\r\n",
        "    seq_len, _, _ = pred_traj.size()\r\n",
        "    loss = pred_traj_gt.permute(1, 0, 2) - pred_traj.permute(1, 0, 2)\r\n",
        "    loss = loss ** 2\r\n",
        "    if consider_ped is not None:\r\n",
        "        loss = torch.sqrt(loss.sum(dim=2)).sum(dim=1) * consider_ped\r\n",
        "    else:\r\n",
        "        loss = torch.sqrt(loss.sum(dim=2)).sum(dim=1)\r\n",
        "    if mode == 'sum':\r\n",
        "        return torch.sum(loss)\r\n",
        "    elif mode == 'raw':\r\n",
        "        return loss\r\n",
        "\r\n",
        "\r\n",
        "def final_displacement_error(\r\n",
        "        pred_pos, pred_pos_gt, consider_ped=None, mode='sum'\r\n",
        "):\r\n",
        "    loss = pred_pos_gt - pred_pos\r\n",
        "    loss = loss ** 2\r\n",
        "    if consider_ped is not None:\r\n",
        "        loss = torch.sqrt(loss.sum(dim=1)) * consider_ped\r\n",
        "    else:\r\n",
        "        loss = torch.sqrt(loss.sum(dim=1))\r\n",
        "    if mode == 'raw':\r\n",
        "        return loss\r\n",
        "    else:\r\n",
        "        return torch.sum(loss)\r\n",
        "\r\n",
        "\r\n",
        "def mean_speed_error(real_speed, fake_speed):\r\n",
        "    # Mean speed loss over all timesteps - Used only for feedback and not for training the model\r\n",
        "    speed_loss = torch.abs(real_speed - fake_speed)\r\n",
        "    add_loss = torch.sum(speed_loss, dim=1)\r\n",
        "    add_loss_1 = torch.sum(add_loss)\r\n",
        "    return add_loss_1\r\n",
        "\r\n",
        "\r\n",
        "def final_speed_error(real_speed, fake_speed):\r\n",
        "    # Final traj speed loss - Used only for feedback and not for training the model\r\n",
        "    speed_loss = torch.abs(real_speed - fake_speed)\r\n",
        "    add_loss_1 = torch.sum(speed_loss)\r\n",
        "    return add_loss_1\r\n",
        "\r\n",
        "\r\n",
        "def relative_to_abs(rel_traj, start_pos):\r\n",
        "    rel_traj = rel_traj.permute(1, 0, 2)\r\n",
        "    displacement = torch.cumsum(rel_traj, dim=1)\r\n",
        "    start_pos = torch.unsqueeze(start_pos, dim=1)\r\n",
        "    abs_traj = displacement + start_pos\r\n",
        "    return abs_traj.permute(1, 0, 2)\r\n",
        "\r\n",
        "\r\n",
        "def get_dataset_name(path):\r\n",
        "    dataset_name = os.path.basename(os.path.dirname(path))\r\n",
        "    return dataset_name\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ZvLBg4FDoU"
      },
      "source": [
        "# PREPROCESSING"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rjWnTY2E5Zb"
      },
      "source": [
        "import os\r\n",
        "import math\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset\r\n",
        "\r\n",
        "\r\n",
        "def data_loader(path, metric, train_or_val):\r\n",
        "    dset = TrajectoryDataset(\r\n",
        "        path,\r\n",
        "        metric, train_or_val)\r\n",
        "\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        loader = DataLoader(dset, batch_size=BATCH_MULTI_CONDITION, shuffle=True, num_workers=NUM_WORKERS, collate_fn=seq_collate)\r\n",
        "    else:\r\n",
        "        loader = DataLoader(dset, batch_size=BATCH_SINGLE_CONDITION, shuffle=True, num_workers=NUM_WORKERS, collate_fn=seq_collate)\r\n",
        "    return dset, loader\r\n",
        "\r\n",
        "\r\n",
        "def seq_collate(data):\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        (obs_seq_list, pred_seq_list, obs_seq_rel_list, pred_seq_rel_list, loss_mask_list, obs_obj_abs_speed,\r\n",
        "        pred_obj_abs_speed, obs_label, pred_label, obs_obj_rel_speed) = zip(*data)\r\n",
        "    else:\r\n",
        "        (obs_seq_list, pred_seq_list, obs_seq_rel_list, pred_seq_rel_list, loss_mask_list, obs_obj_abs_speed,\r\n",
        "         pred_obj_abs_speed, obs_obj_rel_speed) = zip(*data)\r\n",
        "\r\n",
        "    _len = [len(seq) for seq in obs_seq_list]\r\n",
        "    cum_start_idx = [0] + np.cumsum(_len).tolist()\r\n",
        "    seq_start_end = [[start, end]\r\n",
        "                     for start, end in zip(cum_start_idx, cum_start_idx[1:])]\r\n",
        "    obs_traj = torch.cat(obs_seq_list, dim=0).permute(2, 0, 1)\r\n",
        "    pred_traj = torch.cat(pred_seq_list, dim=0).permute(2, 0, 1)\r\n",
        "    obs_traj_rel = torch.cat(obs_seq_rel_list, dim=0).permute(2, 0, 1)\r\n",
        "    pred_traj_rel = torch.cat(pred_seq_rel_list, dim=0).permute(2, 0, 1)\r\n",
        "    obs_obj_abs_speed = torch.cat(obs_obj_abs_speed, dim=0).permute(2, 0, 1)\r\n",
        "    pred_obj_abs_speed = torch.cat(pred_obj_abs_speed, dim=0).permute(2, 0, 1)\r\n",
        "    seq_start_end = torch.LongTensor(seq_start_end)\r\n",
        "    loss_mask = torch.cat(loss_mask_list, dim=0)\r\n",
        "    obs_obj_rel_speed = torch.cat(obs_obj_rel_speed, dim=0).permute(2, 0, 1)\r\n",
        "\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        obs_label = torch.cat(obs_label, dim=0).permute(2, 0, 1)\r\n",
        "        pred_label = torch.cat(pred_label, dim=0).permute(2, 0, 1)\r\n",
        "\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        out = [\r\n",
        "            obs_traj, pred_traj, obs_traj_rel, pred_traj_rel, loss_mask, seq_start_end, obs_obj_abs_speed,\r\n",
        "            pred_obj_abs_speed, obs_label, pred_label, obs_obj_rel_speed\r\n",
        "        ]\r\n",
        "    else:\r\n",
        "        out = [\r\n",
        "            obs_traj, pred_traj, obs_traj_rel, pred_traj_rel, loss_mask, seq_start_end, obs_obj_abs_speed,\r\n",
        "            pred_obj_abs_speed, obs_obj_rel_speed\r\n",
        "        ]\r\n",
        "\r\n",
        "    return tuple(out)\r\n",
        "\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "    return 1 / (1 + math.exp(-x))\r\n",
        "\r\n",
        "\r\n",
        "def read_file(_path):\r\n",
        "    data = []\r\n",
        "    i = 0\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        with open(_path, 'r') as f:\r\n",
        "            for line in f:\r\n",
        "                if i == 0:\r\n",
        "                    i += 1\r\n",
        "                    continue\r\n",
        "                line = line.strip().split(',')\r\n",
        "                line = [i for i in line]\r\n",
        "                data.append(line)\r\n",
        "    else:\r\n",
        "        with open(_path, 'r') as f:\r\n",
        "            for line in f:\r\n",
        "                line = line.strip().split('\\t')\r\n",
        "                line = [float(i) for i in line]\r\n",
        "                data.append(line)\r\n",
        "    return np.asarray(data)\r\n",
        "\r\n",
        "\r\n",
        "def get_min_max_distance(seq_len, all_files):\r\n",
        "    ped_speed = []\r\n",
        "    for path in all_files:\r\n",
        "        data = read_file(path)\r\n",
        "        frames = np.unique(data[:, 0]).tolist()\r\n",
        "        frame_data = []\r\n",
        "        for frame in frames:\r\n",
        "            frame_data.append(data[frame == data[:, 0], :5])\r\n",
        "        num_sequences = int(math.ceil((len(frames) - seq_len + 1)))\r\n",
        "\r\n",
        "        for idx in range(0, num_sequences):\r\n",
        "            curr_seq_data = np.concatenate(frame_data[idx:idx + seq_len], axis=0)\r\n",
        "            obj_in_curr_seq = np.unique(curr_seq_data[:, 1])\r\n",
        "            for _, obj_id in enumerate(obj_in_curr_seq):\r\n",
        "                curr_obj_seq = curr_seq_data[curr_seq_data[:, 1] == obj_id, :]\r\n",
        "                pad_front = frames.index(curr_obj_seq[0, 0]) - idx\r\n",
        "                pad_end = frames.index(curr_obj_seq[-1, 0]) - idx + 1\r\n",
        "                label = curr_obj_seq[0, 2]\r\n",
        "                if pad_end - pad_front != seq_len:\r\n",
        "                    continue\r\n",
        "                curr_obj_x_axis_new = [0.0] + [np.square(t - s) for s, t in\r\n",
        "                                               zip(curr_obj_seq[:, 2], curr_obj_seq[1:, 2])]\r\n",
        "                curr_obj_y_axis_new = [0.0] + [np.square(t - s) for s, t in\r\n",
        "                                               zip(curr_obj_seq[:, 3], curr_obj_seq[1:, 3])]\r\n",
        "\r\n",
        "                curr_obj_dist = np.sqrt(np.add(curr_obj_x_axis_new, curr_obj_y_axis_new))\r\n",
        "                curr_obj_speed = curr_obj_dist / 0.4\r\n",
        "                ped_speed.append(np.max(curr_obj_speed))\r\n",
        "                ped_speed.append(np.min(curr_obj_speed))\r\n",
        "    ped_speed = np.array(ped_speed).reshape(-1, 1)\r\n",
        "    # Find the domain-wise max and min speed to normalize the speed values\r\n",
        "    max_ped_speed = np.amax(ped_speed)\r\n",
        "    unique, counts = np.unique(ped_speed, return_counts=True)\r\n",
        "    a = np.asarray((unique, counts)).T\r\n",
        "    for b in a:\r\n",
        "        print(b)\r\n",
        "    return max_ped_speed\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class TrajectoryDataset(Dataset):\r\n",
        "    \"\"\"Dataloder for the Trajectory datasets\"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self, data_dir, metric=0, train_or_val = None\r\n",
        "    ):\r\n",
        "        super(TrajectoryDataset, self).__init__()\r\n",
        "\r\n",
        "        self.data_dir = data_dir\r\n",
        "        SEQ_LEN = OBS_LEN + PRED_LEN\r\n",
        "        self.train_or_test = metric\r\n",
        "        self.train_or_val = train_or_val\r\n",
        "\r\n",
        "        all_files = os.listdir(self.data_dir)\r\n",
        "        all_files = [os.path.join(self.data_dir, _path) for _path in all_files]\r\n",
        "        #max_ped_speed = get_min_max_distance(SEQ_LEN, all_files)\r\n",
        "        num_obj_in_seq = []\r\n",
        "        seq_list = []\r\n",
        "        seq_list_rel = []\r\n",
        "        obj_abs_speed = []\r\n",
        "        obj_rel_speed = []\r\n",
        "        obj_label = []\r\n",
        "        loss_mask_list = []\r\n",
        "        for path in all_files:\r\n",
        "            data = read_file(path)\r\n",
        "            frames = np.unique(data[:, 0]).tolist()\r\n",
        "            frame_data = []\r\n",
        "            for frame in frames:\r\n",
        "                frame_data.append(data[frame == data[:, 0], :])\r\n",
        "            num_sequences = int(math.ceil((len(frames) - SEQ_LEN + 1)))\r\n",
        "\r\n",
        "            for idx in range(0, num_sequences + 1):\r\n",
        "                curr_seq_data = np.concatenate(\r\n",
        "                    frame_data[idx:idx + SEQ_LEN], axis=0)\r\n",
        "\r\n",
        "                obj_in_curr_seq = np.unique(curr_seq_data[:, 1])\r\n",
        "                curr_loss_mask = np.zeros((len(obj_in_curr_seq), SEQ_LEN))\r\n",
        "                curr_seq_rel = np.zeros((len(obj_in_curr_seq), 2, SEQ_LEN))\r\n",
        "                curr_seq_rel_speed = np.zeros((len(obj_in_curr_seq), SEQ_LEN))\r\n",
        "                curr_seq = np.zeros((len(obj_in_curr_seq), 2, SEQ_LEN))\r\n",
        "                _curr_obj_abs_speed = np.zeros((len(obj_in_curr_seq), SEQ_LEN))\r\n",
        "                _curr_obj_label = np.zeros((len(obj_in_curr_seq), SEQ_LEN))\r\n",
        "                num_obj_considered = 0\r\n",
        "\r\n",
        "                for _, obj_id in enumerate(obj_in_curr_seq):\r\n",
        "                    curr_obj_seq = curr_seq_data[curr_seq_data[:, 1] == obj_id, :]\r\n",
        "                    if MULTI_CONDITIONAL_MODEL:\r\n",
        "                        label = curr_obj_seq[0, 2]\r\n",
        "                    pad_front = frames.index(curr_obj_seq[0, 0]) - idx\r\n",
        "                    pad_end = frames.index(curr_obj_seq[-1, 0]) - idx + 1\r\n",
        "                    if pad_end - pad_front != SEQ_LEN:\r\n",
        "                        continue\r\n",
        "                    if MULTI_CONDITIONAL_MODEL:\r\n",
        "                        if len(curr_obj_seq[:, 0]) != SEQ_LEN:\r\n",
        "                            continue\r\n",
        "                    if MULTI_CONDITIONAL_MODEL:\r\n",
        "                        curr_obj_x_axis_new = [0.0] + [np.square(float(t) - float(s)) for s, t in\r\n",
        "                                                   zip(curr_obj_seq[:, 3], curr_obj_seq[1:, 3])]\r\n",
        "                        curr_obj_y_axis_new = [0.0] + [np.square(float(t) - float(s)) for s, t in\r\n",
        "                                                   zip(curr_obj_seq[:, 4], curr_obj_seq[1:, 4])]\r\n",
        "                    else:\r\n",
        "                        curr_obj_x_axis_new = [np.square(t - s) for s, t in\r\n",
        "                                                       zip(curr_obj_seq[:, 2], curr_obj_seq[1:, 2])]\r\n",
        "                        curr_obj_y_axis_new = [np.square(t - s) for s, t in\r\n",
        "                                                       zip(curr_obj_seq[:, 3], curr_obj_seq[1:, 3])]\r\n",
        "\r\n",
        "                    curr_obj_dist = np.sqrt(np.add(curr_obj_x_axis_new, curr_obj_y_axis_new))\r\n",
        "                    curr_obj_dist = np.insert(curr_obj_dist, 0, curr_obj_dist[0])\r\n",
        "                    # As 50 records are available, we need to divide by 0.1 and we multiply by 10 as a normalization factor.\r\n",
        "                    # For faster computing, we skip that step and directly pass through sigmoid layer\r\n",
        "                    if SINGLE_CONDITIONAL_MODEL:\r\n",
        "                        curr_obj_abs_speed = curr_obj_dist / FRAMES_PER_SECOND_SINGLE_CONDITION\r\n",
        "                    else:\r\n",
        "                        curr_obj_abs_speed = curr_obj_dist / (FRAMES_PER_SECOND_MULTI_CONDITION * NORMALIZATION_FACTOR)\r\n",
        "                    #if any(0.66 <= i <= 1.32 for i in curr_obj_abs_speed):  #DURING TRAINING PHASE\r\n",
        "                    if any(0.66 > i > 1.32 for i in curr_obj_abs_speed):  # DURING TESTING PHASE\r\n",
        "                        continue\r\n",
        "\r\n",
        "\r\n",
        "                    curr_obj_abs_speed = [sigmoid(x) for x in curr_obj_abs_speed]\r\n",
        "                    curr_obj_abs_speed = np.around(curr_obj_abs_speed, decimals=4)\r\n",
        "                    curr_obj_abs_speed = np.transpose(curr_obj_abs_speed)\r\n",
        "                    _idx = num_obj_considered\r\n",
        "\r\n",
        "                    if MULTI_CONDITIONAL_MODEL:\r\n",
        "                        if label == 'AV':\r\n",
        "                            embedding_label = 0.1\r\n",
        "                        elif label == 'OTHERS':\r\n",
        "                            embedding_label = 0.2\r\n",
        "                        elif label == 'AGENT':\r\n",
        "                            embedding_label = 0.3\r\n",
        "                        curr_obj_seq = np.transpose(curr_obj_seq[:, 3:5])\r\n",
        "                        _curr_obj_label[_idx, pad_front:pad_end] = embedding_label\r\n",
        "                    else:\r\n",
        "                        curr_obj_seq = np.transpose(curr_obj_seq[:, 2:])\r\n",
        "                    curr_obj_seq = curr_obj_seq.astype(float)\r\n",
        "                    curr_obj_seq = np.around(curr_obj_seq, decimals=4)\r\n",
        "\r\n",
        "                    rel_curr_obj_seq = np.zeros(curr_obj_seq.shape)\r\n",
        "                    rel_curr_obj_seq[:, 1:] = curr_obj_seq[:, 1:] - curr_obj_seq[:, :-1]\r\n",
        "                    curr_seq[_idx, :, pad_front:pad_end] = curr_obj_seq\r\n",
        "                    curr_seq_rel[_idx, :, pad_front:pad_end] = rel_curr_obj_seq\r\n",
        "\r\n",
        "                    rel_curr_obj_speed = np.zeros(curr_obj_abs_speed.shape)\r\n",
        "                    rel_curr_obj_speed[1:] = curr_obj_abs_speed[1:] - curr_obj_abs_speed[:-1]\r\n",
        "\r\n",
        "                    curr_loss_mask[_idx, pad_front:pad_end] = 1\r\n",
        "                    _curr_obj_abs_speed[_idx, pad_front:pad_end] = curr_obj_abs_speed\r\n",
        "                    curr_seq_rel_speed[_idx, pad_front:pad_end] = rel_curr_obj_speed\r\n",
        "                    num_obj_considered += 1\r\n",
        "\r\n",
        "                if num_obj_considered > 1:\r\n",
        "                    num_obj_in_seq.append(num_obj_considered)\r\n",
        "                    loss_mask_list.append(curr_loss_mask[:num_obj_considered])\r\n",
        "                    obj_abs_speed.append(_curr_obj_abs_speed[:num_obj_considered])\r\n",
        "                    if MULTI_CONDITIONAL_MODEL:\r\n",
        "                        obj_label.append(_curr_obj_label[:num_obj_considered])\r\n",
        "                    seq_list.append(curr_seq[:num_obj_considered])\r\n",
        "                    seq_list_rel.append(curr_seq_rel[:num_obj_considered])\r\n",
        "                    obj_rel_speed.append(curr_seq_rel_speed[:num_obj_considered])\r\n",
        "\r\n",
        "        self.num_seq = len(seq_list)\r\n",
        "        seq_list = np.concatenate(seq_list, axis=0)\r\n",
        "        seq_list_rel = np.concatenate(seq_list_rel, axis=0)\r\n",
        "        obj_abs_speed = np.concatenate(obj_abs_speed, axis=0)\r\n",
        "        obj_rel_speed = np.concatenate(obj_rel_speed, axis=0)\r\n",
        "        loss_mask_list = np.concatenate(loss_mask_list, axis=0)\r\n",
        "        obj_abs_speed = torch.from_numpy(obj_abs_speed).type(torch.float)\r\n",
        "        obj_rel_speed = torch.from_numpy(obj_rel_speed).type(torch.float)\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            obj_label = np.concatenate(obj_label, axis=0)\r\n",
        "\r\n",
        "        # Convert numpy -> Torch Tensor\r\n",
        "        self.obs_traj = torch.from_numpy(\r\n",
        "            seq_list[:, :, :OBS_LEN]).type(torch.float)\r\n",
        "        self.pred_traj = torch.from_numpy(\r\n",
        "            seq_list[:, :, OBS_LEN:]).type(torch.float)\r\n",
        "        self.obs_traj_rel = torch.from_numpy(\r\n",
        "            seq_list_rel[:, :, :OBS_LEN]).type(torch.float)\r\n",
        "        self.pred_traj_rel = torch.from_numpy(\r\n",
        "            seq_list_rel[:, :, OBS_LEN:]).type(torch.float)\r\n",
        "\r\n",
        "        self.obs_obj_abs_speed = obj_abs_speed[:, :OBS_LEN].unsqueeze(dim=1).type(torch.float)\r\n",
        "        self.pred_obj_abs_speed = obj_abs_speed[:, OBS_LEN:].unsqueeze(dim=1).type(torch.float)\r\n",
        "\r\n",
        "        self.obs_obj_rel_speed = obj_rel_speed[:, :OBS_LEN].unsqueeze(dim=1).type(torch.float)\r\n",
        "        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\r\n",
        "\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            self.obs_obj_label = torch.from_numpy(obj_label[:, :OBS_LEN]).unsqueeze(dim=1).type(torch.float)\r\n",
        "            self.pred_obj_label = torch.from_numpy(obj_label[:, OBS_LEN:]).unsqueeze(dim=1).type(torch.float)\r\n",
        "\r\n",
        "        cum_start_idx = [0] + np.cumsum(num_obj_in_seq).tolist()\r\n",
        "        self.seq_start_end = [\r\n",
        "            (start, end)\r\n",
        "            for start, end in zip(cum_start_idx, cum_start_idx[1:])\r\n",
        "        ]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.num_seq\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        start, end = self.seq_start_end[index]\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            out = [\r\n",
        "                self.obs_traj[start:end, :], self.pred_traj[start:end, :],\r\n",
        "                self.obs_traj_rel[start:end, :], self.pred_traj_rel[start:end, :],\r\n",
        "                self.loss_mask[start:end, :], self.obs_obj_abs_speed[start:end, :],\r\n",
        "                self.pred_obj_abs_speed[start:end, :], self.obs_obj_label[start:end, :],\r\n",
        "                self.pred_obj_label[start:end, :], self.obs_obj_rel_speed[start:end, :]\r\n",
        "            ]\r\n",
        "        else:\r\n",
        "            out = [\r\n",
        "                self.obs_traj[start:end, :], self.pred_traj[start:end, :],\r\n",
        "                self.obs_traj_rel[start:end, :], self.pred_traj_rel[start:end, :],\r\n",
        "                self.loss_mask[start:end, :], self.obs_obj_abs_speed[start:end, :],\r\n",
        "                self.pred_obj_abs_speed[start:end, :], self.obs_obj_rel_speed[start:end, :]\r\n",
        "            ]\r\n",
        "        return out"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUY3YnSgE8jz"
      },
      "source": [
        "# MODEL ARCHITECTURE"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HlaJaalByHo"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import math\r\n",
        "from scipy.spatial.distance import pdist, squareform\r\n",
        "import numpy as np\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "\r\n",
        "def make_mlp(dim_list, activation='leakyrelu', batch_norm=True, dropout=0):\r\n",
        "    layers = []\r\n",
        "    for dim_in, dim_out in zip(dim_list[:-1], dim_list[1:]):\r\n",
        "        layers.append(nn.Linear(dim_in, dim_out))\r\n",
        "        if batch_norm:\r\n",
        "            layers.append(nn.BatchNorm1d(dim_out))\r\n",
        "        if activation == 'relu':\r\n",
        "            layers.append(nn.ReLU())\r\n",
        "        elif activation == 'leakyrelu':\r\n",
        "            layers.append(nn.LeakyReLU())\r\n",
        "        elif activation == 'sigmoid':\r\n",
        "            layers.append(nn.Sigmoid())\r\n",
        "        if dropout > 0:\r\n",
        "            layers.append(nn.Dropout(p=dropout))\r\n",
        "    return nn.Sequential(*layers)\r\n",
        "\r\n",
        "\r\n",
        "class SpeedEncoderDecoder(nn.Module):\r\n",
        "    def __init__(self, h_dim):\r\n",
        "        super(SpeedEncoderDecoder, self).__init__()\r\n",
        "\r\n",
        "        self.embedding_dim = EMBEDDING_DIM\r\n",
        "        self.num_layers = NUM_LAYERS\r\n",
        "        self.h_dim = h_dim\r\n",
        "\r\n",
        "        self.speed_decoder = nn.LSTM(EMBEDDING_DIM, h_dim, NUM_LAYERS, dropout=DROPOUT)\r\n",
        "        self.speed_mlp = nn.Linear(h_dim, 1)\r\n",
        "        self.speed_embedding = nn.Linear(1, EMBEDDING_DIM)\r\n",
        "\r\n",
        "    def init_hidden(self, batch):\r\n",
        "        if USE_GPU:\r\n",
        "            c_s, r_s = torch.zeros(self.num_layers, batch, self.h_dim).cuda(), torch.zeros(self.num_layers, batch, self.h_dim).cuda()\r\n",
        "        else:\r\n",
        "            c_s, r_s = torch.zeros(self.num_layers, batch, self.h_dim), torch.zeros(self.num_layers, batch, self.h_dim)\r\n",
        "        return c_s, r_s\r\n",
        "\r\n",
        "    def forward(self, obs_speed, final_enc_h, label=None):\r\n",
        "        sig_layer = nn.Sigmoid()\r\n",
        "        batch = obs_speed.size(1)\r\n",
        "        pred_speed_fake = []\r\n",
        "        final_enc_h = final_enc_h.view(-1, self.h_dim)\r\n",
        "        next_speed = obs_speed[-1, :, :]\r\n",
        "        decoder_input = self.speed_embedding(next_speed.view(-1, 1))\r\n",
        "        decoder_input = decoder_input.view(1, batch, self.embedding_dim)\r\n",
        "\r\n",
        "        decoder_h = final_enc_h.unsqueeze(dim=0)  # INITIALIZE THE DECODER HIDDEN STATE\r\n",
        "        if USE_GPU:\r\n",
        "            decoder_c = torch.zeros(self.num_layers, batch, self.h_dim).cuda()\r\n",
        "        else:\r\n",
        "            decoder_c = torch.zeros(self.num_layers, batch, self.h_dim)\r\n",
        "\r\n",
        "        state_tuple = (decoder_h, decoder_c)  # INITIALIZE THE STATE TUPLES\r\n",
        "\r\n",
        "        for id in range(PRED_LEN):\r\n",
        "            output, state_tuple = self.speed_decoder(decoder_input, state_tuple)\r\n",
        "            next_dec_speed = self.speed_mlp(output.view(-1, self.h_dim))\r\n",
        "            next_speed = sig_layer(next_dec_speed.view(-1, 1))\r\n",
        "            decoder_input = self.speed_embedding(next_speed.view(-1, 1))\r\n",
        "            decoder_input = decoder_input.view(1, batch, self.embedding_dim)\r\n",
        "\r\n",
        "            pred_speed_fake.append(next_speed.view(-1, 1))\r\n",
        "\r\n",
        "        pred_speed_fake = torch.stack(pred_speed_fake, dim=0)\r\n",
        "        return pred_speed_fake\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, h_dim, mlp_input_dim):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "\r\n",
        "        self.mlp_dim = MLP_DIM\r\n",
        "        self.h_dim = h_dim\r\n",
        "        self.embedding_dim = EMBEDDING_DIM\r\n",
        "        self.num_layers = NUM_LAYERS\r\n",
        "        self.mlp_input_dim = mlp_input_dim\r\n",
        "\r\n",
        "        self.encoder = nn.LSTM(EMBEDDING_DIM, h_dim, NUM_LAYERS, dropout=DROPOUT)\r\n",
        "\r\n",
        "        self.spatial_embedding = nn.Linear(mlp_input_dim, EMBEDDING_DIM)\r\n",
        "\r\n",
        "    def init_hidden(self, batch):\r\n",
        "        if USE_GPU:\r\n",
        "            c_s, r_s = torch.zeros(self.num_layers, batch, self.h_dim).cuda(), torch.zeros(self.num_layers, batch, self.h_dim).cuda()\r\n",
        "        else:\r\n",
        "            c_s, r_s = torch.zeros(self.num_layers, batch, self.h_dim), torch.zeros(self.num_layers, batch, self.h_dim)\r\n",
        "        return c_s, r_s\r\n",
        "\r\n",
        "    def forward(self, obs_traj, obs_ped_speed, label=None):\r\n",
        "        batch = obs_traj.size(1)\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            embedding_input = torch.cat([obs_traj, obs_ped_speed, label], dim=2)\r\n",
        "        else:\r\n",
        "            embedding_input = torch.cat([obs_traj, obs_ped_speed], dim=2)\r\n",
        "        traj_speed_embedding = self.spatial_embedding(embedding_input.contiguous().view(-1, self.mlp_input_dim))\r\n",
        "        obs_traj_embedding = traj_speed_embedding.view(-1, batch, self.embedding_dim)\r\n",
        "        state_tuple = self.init_hidden(batch)\r\n",
        "        output, state = self.encoder(obs_traj_embedding, state_tuple)\r\n",
        "        final_h = state[0]\r\n",
        "        return final_h\r\n",
        "\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "    return 1 / (1 + math.exp(-x))\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, h_dim, mlp_input_dim):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "\r\n",
        "        self.mlp_dim = MLP_DIM\r\n",
        "        self.h_dim = h_dim\r\n",
        "        self.embedding_dim = EMBEDDING_DIM\r\n",
        "        self.mlp_input_dim = mlp_input_dim\r\n",
        "\r\n",
        "        self.decoder = nn.LSTM(EMBEDDING_DIM, h_dim, NUM_LAYERS, dropout=DROPOUT)\r\n",
        "\r\n",
        "        self.spatial_embedding = nn.Linear(mlp_input_dim, EMBEDDING_DIM)\r\n",
        "        self.hidden2pos = nn.Linear(h_dim, 2)\r\n",
        "\r\n",
        "    def forward(self, last_pos, last_pos_rel, state_tuple, seq_start_end, pred_ped_speed, train_or_test, fake_ped_speed, label=None):\r\n",
        "        batch = last_pos.size(0)\r\n",
        "        pred_traj_fake_rel = []\r\n",
        "        if train_or_test == 0:\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                last_pos_speed = torch.cat([last_pos_rel, pred_ped_speed[0, :, :], label[0, :, :]], dim=1)\r\n",
        "            else:\r\n",
        "                last_pos_speed = torch.cat([last_pos_rel, pred_ped_speed[0, :, :]], dim=1)\r\n",
        "        elif train_or_test == 1:  # USED FOR PREDICTION PURPOSE\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                last_pos_speed = torch.cat([last_pos_rel, fake_ped_speed[0, :, :], label[0, :, :]], dim=1)\r\n",
        "            else:\r\n",
        "                last_pos_speed = torch.cat([last_pos_rel, fake_ped_speed[0, :, :]], dim=1)\r\n",
        "        else:  # USED FOR SIMULATION PURPOSE\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                next_speed = speed_control(pred_ped_speed[0, :, :], seq_start_end, label=label[0, :, :])\r\n",
        "                last_pos_speed = torch.cat([last_pos_rel, next_speed, label[0, :, :]], dim=1)\r\n",
        "            else:\r\n",
        "                next_speed = speed_control(pred_ped_speed[0, :, :], seq_start_end)\r\n",
        "                last_pos_speed = torch.cat([last_pos_rel, next_speed], dim=1)\r\n",
        "        decoder_input = self.spatial_embedding(last_pos_speed)\r\n",
        "        decoder_input = decoder_input.view(1, batch, self.embedding_dim)\r\n",
        "\r\n",
        "        for id in range(PRED_LEN):\r\n",
        "            output, state_tuple = self.decoder(decoder_input, state_tuple)\r\n",
        "            rel_pos = self.hidden2pos(output.view(-1, self.h_dim))\r\n",
        "            curr_pos = rel_pos + last_pos\r\n",
        "            if id + 1 != PRED_LEN:\r\n",
        "                if train_or_test == 0:\r\n",
        "                    speed = pred_ped_speed[id + 1, :, :]\r\n",
        "                    if MULTI_CONDITIONAL_MODEL:\r\n",
        "                        curr_label = label[0, :, :]\r\n",
        "                elif train_or_test == 1:\r\n",
        "                    speed = fake_ped_speed[id + 1, :, :]\r\n",
        "                    if MULTI_CONDITIONAL_MODEL:\r\n",
        "                        curr_label = label[0, :, :]\r\n",
        "                else:\r\n",
        "                    if SINGLE_CONDITIONAL_MODEL:\r\n",
        "                        speed = speed_control(pred_ped_speed[id, :, :], seq_start_end, id=id+1)\r\n",
        "                    elif MULTI_CONDITIONAL_MODEL:\r\n",
        "                        curr_label = label[0, :, :]\r\n",
        "                        speed = speed_control(pred_ped_speed[0, :, :], seq_start_end, label=curr_label)\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                decoder_input = torch.cat([rel_pos, speed, curr_label], dim=1)\r\n",
        "            else:\r\n",
        "                decoder_input = torch.cat([rel_pos, speed], dim=1)\r\n",
        "            decoder_input = self.spatial_embedding(decoder_input)\r\n",
        "            decoder_input = decoder_input.view(1, batch, self.embedding_dim)\r\n",
        "\r\n",
        "            pred_traj_fake_rel.append(rel_pos.view(batch, -1))\r\n",
        "            last_pos = curr_pos\r\n",
        "\r\n",
        "        pred_traj_fake_rel = torch.stack(pred_traj_fake_rel, dim=0)\r\n",
        "        return pred_traj_fake_rel\r\n",
        "\r\n",
        "\r\n",
        "class PoolingModule(nn.Module):\r\n",
        "    \"\"\"Todo\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, h_dim, mlp_input_dim):\r\n",
        "        super(PoolingModule, self).__init__()\r\n",
        "        self.mlp_dim = MLP_DIM\r\n",
        "        self.h_dim = h_dim\r\n",
        "        self.bottleneck_dim = BOTTLENECK_DIM\r\n",
        "        self.embedding_dim = EMBEDDING_DIM\r\n",
        "        self.mlp_input_dim = mlp_input_dim\r\n",
        "\r\n",
        "        mlp_pre_dim = self.embedding_dim + self.h_dim\r\n",
        "        mlp_pre_pool_dims = [mlp_pre_dim, 512, BOTTLENECK_DIM]\r\n",
        "\r\n",
        "        self.pos_embedding = nn.Linear(2, EMBEDDING_DIM)\r\n",
        "        self.mlp_pre_pool = make_mlp(mlp_pre_pool_dims, activation=ACTIVATION_RELU, batch_norm=BATCH_NORM, dropout=DROPOUT)\r\n",
        "\r\n",
        "    def forward(self, h_states, seq_start_end, train_or_test, last_pos, label=None):\r\n",
        "        pool_h = []\r\n",
        "        for _, (start, end) in enumerate(seq_start_end):\r\n",
        "            start = start.item()\r\n",
        "            end = end.item()\r\n",
        "            num_ped = end - start\r\n",
        "            curr_hidden_ped = h_states.view(-1, self.h_dim)[start:end]\r\n",
        "            repeat_hstate = curr_hidden_ped.repeat(num_ped, 1).view(num_ped, num_ped, -1)\r\n",
        "            feature = last_pos[start:end]\r\n",
        "            curr_end_pos_1 = feature.repeat(num_ped, 1)\r\n",
        "            curr_end_pos_2 = feature.unsqueeze(dim=1).repeat(1, num_ped, 1).view(-1, 2)\r\n",
        "            social_features = curr_end_pos_1[:, :2] - curr_end_pos_2[:, :2]\r\n",
        "            position_feature_embedding = self.pos_embedding(social_features.contiguous().view(-1, 2))\r\n",
        "            pos_mlp_input = torch.cat(\r\n",
        "                [repeat_hstate.view(-1, self.h_dim), position_feature_embedding.view(-1, self.embedding_dim)], dim=1)\r\n",
        "            pos_attn_h = self.mlp_pre_pool(pos_mlp_input)\r\n",
        "            curr_pool_h = pos_attn_h.view(num_ped, num_ped, -1).max(1)[0]\r\n",
        "            pool_h.append(curr_pool_h)\r\n",
        "        pool_h = torch.cat(pool_h, dim=0)\r\n",
        "        return pool_h\r\n",
        "\r\n",
        "\r\n",
        "class AggregationModule(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, h_dim, mlp_input_dim):\r\n",
        "        super(AggregationModule, self).__init__()\r\n",
        "        self.mlp_dim = MLP_DIM\r\n",
        "        self.h_dim = h_dim\r\n",
        "        self.bottleneck_dim = BOTTLENECK_DIM\r\n",
        "        self.embedding_dim = EMBEDDING_DIM\r\n",
        "        self.mlp_input_dim = mlp_input_dim\r\n",
        "\r\n",
        "        mlp_pre_dim = self.h_dim * MAX_CONSIDERED_PED\r\n",
        "        mlp_pre_pool_dims = [mlp_pre_dim, 512, BOTTLENECK_DIM]\r\n",
        "\r\n",
        "        self.pos_embedding = nn.Linear(2, EMBEDDING_DIM)\r\n",
        "        self.mlp_pre_pool = make_mlp(mlp_pre_pool_dims, activation=ACTIVATION_RELU, batch_norm=BATCH_NORM, dropout=DROPOUT)\r\n",
        "\r\n",
        "    def forward(self, h_states, seq_start_end, train_or_test, last_pos, label=None):\r\n",
        "        pool_h = []\r\n",
        "        for _, (start, end) in enumerate(seq_start_end):\r\n",
        "            start = start.item()\r\n",
        "            end = end.item()\r\n",
        "            num_ped = end - start\r\n",
        "            curr_hidden_ped = h_states.view(-1, self.h_dim)[start:end]\r\n",
        "\r\n",
        "            feature = last_pos[start:end].cpu().data.numpy()\r\n",
        "            dist = squareform(pdist(feature, metric=\"euclidean\"))\r\n",
        "            idx = np.argsort(dist)\r\n",
        "            req_h_states = []\r\n",
        "            for ids in idx:\r\n",
        "                req_ids = torch.from_numpy(ids).type(torch.cuda.FloatTensor).view(num_ped, 1)\r\n",
        "                new_h_states = torch.cat([curr_hidden_ped, req_ids], dim=1)\r\n",
        "                sorted = new_h_states[new_h_states[:, -1].sort()[1]]\r\n",
        "                required_h_states = sorted[:, :-1].contiguous().view(1, -1)\r\n",
        "                if num_ped >= MAX_CONSIDERED_PED:\r\n",
        "                    req_h_states.append(required_h_states[:, :(MAX_CONSIDERED_PED*self.h_dim)])\r\n",
        "                else:\r\n",
        "                    h_state_zeros = torch.zeros(1, self.h_dim * MAX_CONSIDERED_PED).cuda()\r\n",
        "                    h_state_zeros[:, :self.h_dim * num_ped] = required_h_states\r\n",
        "                    req_h_states.append(h_state_zeros)\r\n",
        "            aggregated_h_states = torch.cat(req_h_states, dim=0)\r\n",
        "            agg_h = self.mlp_pre_pool(aggregated_h_states)\r\n",
        "            pool_h.append(agg_h)\r\n",
        "        pool_h = torch.cat(pool_h, dim=0)\r\n",
        "        return pool_h\r\n",
        "\r\n",
        "\r\n",
        "class AttentionModule(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, h_dim, mlp_input_dim):\r\n",
        "        super(AttentionModule, self).__init__()\r\n",
        "        self.mlp_dim = MLP_DIM\r\n",
        "        self.h_dim = h_dim\r\n",
        "        self.bottleneck_dim = BOTTLENECK_DIM\r\n",
        "        self.embedding_dim = EMBEDDING_DIM\r\n",
        "        self.mlp_input_dim = mlp_input_dim\r\n",
        "\r\n",
        "        mlp_pre_dim = self.h_dim + self.embedding_dim\r\n",
        "        mlp_pre_pool_dims = [mlp_pre_dim, 512, BOTTLENECK_DIM]\r\n",
        "        self.attn = nn.Linear(MAX_CONSIDERED_PED*BOTTLENECK_DIM, MAX_CONSIDERED_PED)\r\n",
        "\r\n",
        "        self.pos_embedding = nn.Linear(2, EMBEDDING_DIM)\r\n",
        "        self.mlp_pre_pool = make_mlp(mlp_pre_pool_dims, activation=ACTIVATION_RELU, batch_norm=BATCH_NORM, dropout=DROPOUT)\r\n",
        "\r\n",
        "    def forward(self, h_states, seq_start_end, train_or_test, last_pos, label=None):\r\n",
        "        pool_h = []\r\n",
        "        for _, (start, end) in enumerate(seq_start_end):\r\n",
        "            start = start.item()\r\n",
        "            end = end.item()\r\n",
        "            num_ped = end - start\r\n",
        "            curr_hidden_ped = h_states.view(-1, self.h_dim)[start:end]\r\n",
        "            repeat_hstate = curr_hidden_ped.repeat(num_ped, 1).view(num_ped, num_ped, -1)\r\n",
        "            feature = last_pos[start:end]\r\n",
        "            curr_end_pos_1 = feature.repeat(num_ped, 1)\r\n",
        "            curr_end_pos_2 = feature.unsqueeze(dim=1).repeat(1, num_ped, 1).view(-1, 2)\r\n",
        "            social_features = curr_end_pos_1[:, :2] - curr_end_pos_2[:, :2]\r\n",
        "            feature = last_pos[start:end].cpu().data.numpy()\r\n",
        "            dist = squareform(pdist(feature, metric=\"euclidean\"))\r\n",
        "            idx = np.argsort(dist)\r\n",
        "            req_h_states = []\r\n",
        "            social_features = social_features.view(num_ped, num_ped, 2)\r\n",
        "            if num_ped < MAX_CONSIDERED_PED:\r\n",
        "                social_feature_embedding = self.pos_embedding(social_features.contiguous().view(-1, 2))\r\n",
        "                h_state_zeros = torch.zeros(num_ped, MAX_CONSIDERED_PED, self.h_dim).cuda()\r\n",
        "                feature_zeros = torch.zeros(num_ped, MAX_CONSIDERED_PED, self.embedding_dim).cuda()\r\n",
        "                h_state_zeros[:num_ped, :num_ped, :] = repeat_hstate.view(num_ped, num_ped, self.h_dim)\r\n",
        "                feature_zeros[:num_ped, :num_ped, :] = social_feature_embedding.view(num_ped, num_ped, self.embedding_dim)\r\n",
        "                concat_features = torch.cat([h_state_zeros.view(-1, self.h_dim), feature_zeros.view(-1, self.embedding_dim)], dim=1)\r\n",
        "            else:\r\n",
        "                for ids, curr_features, curr_h_states in zip(idx, social_features, repeat_hstate):\r\n",
        "                    req_ids = torch.from_numpy(ids).type(torch.cuda.FloatTensor).view(num_ped, 1)\r\n",
        "                    new_h_states = torch.cat([curr_h_states, req_ids], dim=1)\r\n",
        "                    new_features = torch.cat([curr_features, req_ids], dim=1)\r\n",
        "                    h_states_sorted = new_h_states[new_h_states[:, -1].sort()[1]][:MAX_CONSIDERED_PED, :]\r\n",
        "                    features_sorted = new_features[new_features[:, -1].sort()[1]][:MAX_CONSIDERED_PED, :]\r\n",
        "                    required_h_states = h_states_sorted[:, :-1]\r\n",
        "                    required_features = features_sorted[:, :-1]\r\n",
        "                    social_feature_embedding = self.pos_embedding(required_features.contiguous().view(-1, 2))\r\n",
        "                    req_h_states.append(torch.cat([required_h_states, social_feature_embedding], dim=1))\r\n",
        "                concat_features = torch.stack(req_h_states, dim=0)\r\n",
        "            attn_h = self.mlp_pre_pool(concat_features.view(-1, (self.h_dim+self.embedding_dim)))\r\n",
        "            attn_h = attn_h.view(num_ped, MAX_CONSIDERED_PED, -1)\r\n",
        "            attn_w = F.softmax(self.attn(attn_h.view(num_ped, -1)), dim=1)\r\n",
        "            attn_w = attn_w.view(num_ped, MAX_CONSIDERED_PED, 1)\r\n",
        "            attn_h = torch.sum(attn_h * attn_w, dim=1)\r\n",
        "            pool_h.append(attn_h)\r\n",
        "        pool_h = torch.cat(pool_h, dim=0)\r\n",
        "        return pool_h\r\n",
        "\r\n",
        "\r\n",
        "def speed_control(pred_traj_first_speed, seq_start_end, label=None, id=None):\r\n",
        "    \"\"\"This method acts as speed regulator. Using this method, user can add\r\n",
        "    speed at one/more frames, stop the agent and so on\"\"\"\r\n",
        "    for _, (start, end) in enumerate(seq_start_end):\r\n",
        "        start = start.item()\r\n",
        "        end = end.item()\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            av_tensor = [1, 0, 0]\r\n",
        "            av = torch.FloatTensor(av_tensor)\r\n",
        "            other_tensor = [0, 1, 0]\r\n",
        "            other = torch.FloatTensor(other_tensor)\r\n",
        "            agent_tensor = [0, 0, 1]\r\n",
        "            agent = torch.FloatTensor(agent_tensor)\r\n",
        "            if DIFFERENT_SPEED_MULTI_CONDITION:\r\n",
        "                for a, b in zip(range(start, end), label):\r\n",
        "                    if torch.all(torch.eq(b, av)):\r\n",
        "                        pred_traj_first_speed[a] = sigmoid(AV_SPEED * AV_MAX_SPEED)\r\n",
        "                    elif torch.all(torch.eq(b, other)):\r\n",
        "                        pred_traj_first_speed[a] = sigmoid(OTHER_SPEED * OTHER_MAX_SPEED)\r\n",
        "                    elif torch.all(torch.eq(b, agent)):\r\n",
        "                        pred_traj_first_speed[a] = sigmoid(AGENT_SPEED * AGENT_MAX_SPEED)\r\n",
        "            elif CONSTANT_SPEED_MULTI_CONDITION:\r\n",
        "                # To make all pedestrians travel at same and constant speed throughout\r\n",
        "                for a, b in zip(range(start, end), label):\r\n",
        "                    if torch.eq(b, 0.1):\r\n",
        "                        pred_traj_first_speed[a] = sigmoid(CS_MULTI_CONDITION * AV_MAX_SPEED)\r\n",
        "                    elif torch.eq(b, 0.2):\r\n",
        "                        pred_traj_first_speed[a] = sigmoid(CS_MULTI_CONDITION * OTHER_MAX_SPEED)\r\n",
        "                    elif torch.eq(b, 0.3):\r\n",
        "                        pred_traj_first_speed[a] = sigmoid(CS_MULTI_CONDITION * AGENT_MAX_SPEED)\r\n",
        "        elif SINGLE_CONDITIONAL_MODEL:\r\n",
        "            if CONSTANT_SPEED_SINGLE_CONDITION:\r\n",
        "                dataset_name = get_dataset_name(SINGLE_TEST_DATASET_PATH)\r\n",
        "                if dataset_name == 'eth':\r\n",
        "                    speed_to_simulate = ZARA1_MAX_SPEED * CS_SINGLE_CONDITION\r\n",
        "                elif dataset_name == 'hotel':\r\n",
        "                    speed_to_simulate = ETH_MAX_SPEED * CS_SINGLE_CONDITION\r\n",
        "                elif dataset_name == 'univ':\r\n",
        "                    speed_to_simulate = ZARA1_MAX_SPEED * CS_SINGLE_CONDITION\r\n",
        "                elif dataset_name == 'zara1':\r\n",
        "                    speed_to_simulate = ETH_MAX_SPEED * CS_SINGLE_CONDITION\r\n",
        "                elif dataset_name == 'zara2':\r\n",
        "                    speed_to_simulate = ETH_MAX_SPEED * CS_SINGLE_CONDITION\r\n",
        "\r\n",
        "                # To add an additional speed for each pedestrain and every frame\r\n",
        "                for a in range(start, end):\r\n",
        "                    pred_traj_first_speed[a] = sigmoid(speed_to_simulate)\r\n",
        "\r\n",
        "            elif STOP_PED_SINGLE_CONDITION:\r\n",
        "                # To stop all pedestrians\r\n",
        "                for a in range(start, end):\r\n",
        "                    pred_traj_first_speed[a] = sigmoid(0)\r\n",
        "            elif ADD_SPEED_PARTICULAR_FRAME and len(FRAMES_TO_ADD_SPEED) > 0:\r\n",
        "                for a in range(start, end):\r\n",
        "                    # Add speed to particular frame for all pedestrian\r\n",
        "                    sorted_frames = FRAMES_TO_ADD_SPEED\r\n",
        "                    for frames in sorted_frames:\r\n",
        "                        if id == frames:\r\n",
        "                            pred_traj_first_speed[a] = sigmoid(ETH_MAX_SPEED * MAX_SPEED)\r\n",
        "                        else:\r\n",
        "                            pred_traj_first_speed[a] = pred_traj_first_speed[a]\r\n",
        "\r\n",
        "    return pred_traj_first_speed.view(-1, 1)\r\n",
        "\r\n",
        "\r\n",
        "class TrajectoryGenerator(nn.Module):\r\n",
        "    def __init__(self, mlp_dim, h_dim):\r\n",
        "        super(TrajectoryGenerator, self).__init__()\r\n",
        "\r\n",
        "        self.mlp_dim = MLP_DIM\r\n",
        "        self.h_dim = h_dim\r\n",
        "\r\n",
        "        self.mlp_input_dim = mlp_dim\r\n",
        "        self.h_dim = h_dim\r\n",
        "\r\n",
        "        self.embedding_dim = EMBEDDING_DIM\r\n",
        "        self.noise_dim = NOISE_DIM\r\n",
        "        self.num_layers = NUM_LAYERS\r\n",
        "        self.bottleneck_dim = BOTTLENECK_DIM\r\n",
        "\r\n",
        "        self.encoder = Encoder(h_dim=h_dim, mlp_input_dim=mlp_dim)\r\n",
        "        self.decoder = Decoder(h_dim = h_dim, mlp_input_dim=mlp_dim)\r\n",
        "\r\n",
        "        self.noise_first_dim = NOISE_DIM[0]\r\n",
        "\r\n",
        "        if POOLING_TYPE:\r\n",
        "            self.conditionalPoolingModule = PoolingModule(h_dim=h_dim, mlp_input_dim=mlp_dim)\r\n",
        "            mlp_decoder_context_dims = [h_dim + BOTTLENECK_DIM, MLP_DIM, h_dim - self.noise_first_dim]\r\n",
        "        elif AGGREGATION_TYPE:\r\n",
        "            self.aggregation_module = AggregationModule(h_dim=h_dim, mlp_input_dim=mlp_dim)\r\n",
        "            mlp_decoder_context_dims = [h_dim + BOTTLENECK_DIM, MLP_DIM, h_dim - self.noise_first_dim]\r\n",
        "        elif ATTENTION_TYPE:\r\n",
        "            self.attention_module = AttentionModule(h_dim=h_dim, mlp_input_dim=mlp_dim)\r\n",
        "            mlp_decoder_context_dims = [h_dim + BOTTLENECK_DIM, MLP_DIM, h_dim - self.noise_first_dim]\r\n",
        "        else:\r\n",
        "            mlp_decoder_context_dims = [h_dim, MLP_DIM, h_dim - self.noise_first_dim]\r\n",
        "\r\n",
        "        self.mlp_decoder_context = make_mlp(mlp_decoder_context_dims, activation=ACTIVATION_RELU, batch_norm=BATCH_NORM,\r\n",
        "                                            dropout=DROPOUT)\r\n",
        "\r\n",
        "    def add_noise(self, _input, seq_start_end):\r\n",
        "        noise_shape = (seq_start_end.size(0),) + self.noise_dim\r\n",
        "        if USE_GPU:\r\n",
        "            z_decoder = torch.randn(*noise_shape).cuda()\r\n",
        "        else:\r\n",
        "            z_decoder = torch.randn(*noise_shape)\r\n",
        "        _list = []\r\n",
        "        for idx, (start, end) in enumerate(seq_start_end):\r\n",
        "            noise = z_decoder[idx].view(1, -1).repeat(end.item() - start.item(), 1)\r\n",
        "            _list.append(torch.cat([_input[start:end], noise], dim=1))\r\n",
        "        decoder_h = torch.cat(_list, dim=0)\r\n",
        "        return decoder_h\r\n",
        "\r\n",
        "    def forward(self, obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed, pred_traj, train_or_test, fake_ped_speed, obs_obj_rel_speed, obs_label=None, pred_label=None, user_noise=None):\r\n",
        "        batch = obs_traj_rel.size(1)\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            final_encoder_h = self.encoder(obs_traj_rel, obs_ped_speed, label=obs_label)\r\n",
        "        else:\r\n",
        "            final_encoder_h = self.encoder(obs_traj_rel, obs_ped_speed, label=None)\r\n",
        "        if POOLING_TYPE:\r\n",
        "            pm_final_vector = self.conditionalPoolingModule(final_encoder_h, seq_start_end, train_or_test, obs_traj[-1, :, :])\r\n",
        "            mlp_decoder_context_input = torch.cat([final_encoder_h.view(-1, self.h_dim), pm_final_vector], dim=1)\r\n",
        "        elif AGGREGATION_TYPE:\r\n",
        "            agg_final_vector = self.aggregation_module(final_encoder_h, seq_start_end, train_or_test, obs_traj[-1, :, :])\r\n",
        "            mlp_decoder_context_input = torch.cat([final_encoder_h.view(-1, self.h_dim), agg_final_vector], dim=1)\r\n",
        "        elif ATTENTION_TYPE:\r\n",
        "            attn_final_vector = self.attention_module(final_encoder_h, seq_start_end, train_or_test, obs_traj[-1, :, :])\r\n",
        "            mlp_decoder_context_input = torch.cat([final_encoder_h.view(-1, self.h_dim), attn_final_vector], dim=1)\r\n",
        "        else:\r\n",
        "            mlp_decoder_context_input = final_encoder_h.view(-1, self.h_dim)\r\n",
        "        noise_input = self.mlp_decoder_context(mlp_decoder_context_input)\r\n",
        "\r\n",
        "        decoder_h = self.add_noise(noise_input, seq_start_end).unsqueeze(dim=0)\r\n",
        "        if USE_GPU:\r\n",
        "            decoder_c = torch.zeros(self.num_layers, batch, self.h_dim).cuda()\r\n",
        "        else:\r\n",
        "            decoder_c = torch.zeros(self.num_layers, batch, self.h_dim)\r\n",
        "\r\n",
        "        state_tuple = (decoder_h, decoder_c)\r\n",
        "\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            decoder_out = self.decoder(obs_traj[-1], obs_traj_rel[-1], state_tuple, seq_start_end, pred_ped_speed,\r\n",
        "            train_or_test, fake_ped_speed, label=pred_label)\r\n",
        "        else:\r\n",
        "            decoder_out = self.decoder(obs_traj[-1], obs_traj_rel[-1], state_tuple, seq_start_end, pred_ped_speed,\r\n",
        "            train_or_test, fake_ped_speed)\r\n",
        "        pred_traj_fake_rel = decoder_out\r\n",
        "\r\n",
        "        # LOGGING THE OUTPUT FOR MULTI CONDITIONAL MODEL WHEN THE PREDICTED LENGTH IS MORE - useful to check the speed condition\r\n",
        "        if train_or_test == 3:\r\n",
        "            simulated_trajectories = []\r\n",
        "            for _, (start, end) in enumerate(seq_start_end):\r\n",
        "                start = start.item()\r\n",
        "                end = end.item()\r\n",
        "                obs_test_traj = obs_traj[:, start:end, :]\r\n",
        "                pred_test_traj_rel = pred_traj_fake_rel[:, start:end, :]\r\n",
        "                pred_test_traj = relative_to_abs(pred_test_traj_rel, obs_test_traj[-1])\r\n",
        "                speed_added = pred_ped_speed[0, start:end, :]\r\n",
        "                print(speed_added)\r\n",
        "                print(pred_test_traj)\r\n",
        "                simulated_trajectories.append(pred_test_traj)\r\n",
        "        return pred_traj_fake_rel, decoder_h.view(-1, self.h_dim)\r\n",
        "\r\n",
        "\r\n",
        "class TrajectoryDiscriminator(nn.Module):\r\n",
        "    def __init__(self, h_dim, mlp_dim):\r\n",
        "        super(TrajectoryDiscriminator, self).__init__()\r\n",
        "\r\n",
        "        self.encoder = Encoder(h_dim, mlp_input_dim=mlp_dim)\r\n",
        "\r\n",
        "        real_classifier_dims = [h_dim, MLP_DIM, 1]\r\n",
        "        self.real_classifier = make_mlp(real_classifier_dims, activation=ACTIVATION_RELU, batch_norm=BATCH_NORM, dropout=DROPOUT)\r\n",
        "\r\n",
        "    def forward(self, traj, traj_rel, ped_speed, label=None):\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            final_h = self.encoder(traj_rel, ped_speed, label=label)  # final layer of the encoder is returned\r\n",
        "        else:\r\n",
        "            final_h = self.encoder(traj_rel, ped_speed, label=None)  # final layer of the encoder is returned\r\n",
        "        scores = self.real_classifier(final_h.squeeze())  # mlp - 64 --> 1024 --> 1\r\n",
        "        return scores\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9NsAPRNEDWs"
      },
      "source": [
        "def init_weights(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        nn.init.kaiming_normal_(m.weight)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08apALl7ByJ9",
        "outputId": "9d7a77ac-4968-437b-d3e8-6720308e1085"
      },
      "source": [
        "print(\"Process Started\")\r\n",
        "train_path = SINGLE_TRAIN_DATASET_PATH\r\n",
        "val_path = SINGLE_VAL_DATASET_PATH\r\n",
        "print(\"Initializing train dataset\")\r\n",
        "train_dset, train_loader = data_loader(train_path, 0, 'train')\r\n",
        "print(\"Initializing val dataset\")\r\n",
        "_, val_loader = data_loader(val_path, 0, 'val')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process Started\n",
            "Initializing train dataset\n",
            "Initializing val dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ7RzncqByM_",
        "outputId": "d63a4f45-ab37-40f4-ef19-a2f3dc68ccec"
      },
      "source": [
        "iterations_per_epoch = len(train_dset) / BATCH_SINGLE_CONDITION / D_STEPS\r\n",
        "\r\n",
        "NUM_ITERATIONS = int(iterations_per_epoch * NUM_EPOCHS_SINGLE_CONDITION)\r\n",
        "generator = TrajectoryGenerator(mlp_dim=MLP_INPUT_DIM_SINGLE_CONDITION,\r\n",
        "                                h_dim=H_DIM_GENERATOR_SINGLE_CONDITION)\r\n",
        "discriminator = TrajectoryDiscriminator(mlp_dim=MLP_INPUT_DIM_SINGLE_CONDITION,\r\n",
        "                                        h_dim=H_DIM_DISCRIMINATOR_SINGLE_CONDITION)\r\n",
        "speed_regressor = SpeedEncoderDecoder(h_dim=H_DIM_GENERATOR_SINGLE_CONDITION)\r\n",
        "required_epoch = NUM_EPOCHS_SINGLE_CONDITION\r\n",
        "\r\n",
        "print(iterations_per_epoch)\r\n",
        "generator.apply(init_weights)\r\n",
        "generator.type(torch.cuda.FloatTensor).train()\r\n",
        "print('Here is the generator:')\r\n",
        "print(generator)\r\n",
        "\r\n",
        "discriminator.apply(init_weights)\r\n",
        "discriminator.type(torch.cuda.FloatTensor).train()\r\n",
        "print('Here is the discriminator:')\r\n",
        "print(discriminator)\r\n",
        "\r\n",
        "speed_regressor.apply(init_weights)\r\n",
        "speed_regressor.type(torch.cuda.FloatTensor).train()\r\n",
        "print('Here is the Speed Regressor:')\r\n",
        "print(speed_regressor)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56.125\n",
            "Here is the generator:\n",
            "TrajectoryGenerator(\n",
            "  (encoder): Encoder(\n",
            "    (encoder): LSTM(16, 32)\n",
            "    (spatial_embedding): Linear(in_features=3, out_features=16, bias=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (decoder): LSTM(16, 32)\n",
            "    (spatial_embedding): Linear(in_features=3, out_features=16, bias=True)\n",
            "    (hidden2pos): Linear(in_features=32, out_features=2, bias=True)\n",
            "  )\n",
            "  (aggregation_module): AggregationModule(\n",
            "    (pos_embedding): Linear(in_features=2, out_features=16, bias=True)\n",
            "    (mlp_pre_pool): Sequential(\n",
            "      (0): Linear(in_features=160, out_features=512, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=512, out_features=32, bias=True)\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (mlp_decoder_context): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=24, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            ")\n",
            "Here is the discriminator:\n",
            "TrajectoryDiscriminator(\n",
            "  (encoder): Encoder(\n",
            "    (encoder): LSTM(16, 64)\n",
            "    (spatial_embedding): Linear(in_features=3, out_features=16, bias=True)\n",
            "  )\n",
            "  (real_classifier): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            ")\n",
            "Here is the Speed Regressor:\n",
            "SpeedEncoderDecoder(\n",
            "  (speed_decoder): LSTM(16, 32)\n",
            "  (speed_mlp): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (speed_embedding): Linear(in_features=1, out_features=16, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PScBmatByOq"
      },
      "source": [
        "g_loss_fn = gan_g_loss\r\n",
        "d_loss_fn = gan_d_loss\r\n",
        "\r\n",
        "optimizer_g = torch.optim.Adam(generator.parameters(), lr=G_LEARNING_RATE)\r\n",
        "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=D_LEARNING_RATE)\r\n",
        "optimizer_speed_regressor = torch.optim.Adam(speed_regressor.parameters(), lr=D_LEARNING_RATE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXSGVrYAAltA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2n1fyOJBySh"
      },
      "source": [
        "t, epoch = 0, 0\r\n",
        "checkpoint = {\r\n",
        "    'G_losses': defaultdict(list),\r\n",
        "    'D_losses': defaultdict(list),\r\n",
        "    'g_state': None,\r\n",
        "    'g_optim_state': None,\r\n",
        "    'd_state': None,\r\n",
        "    'd_optim_state': None,\r\n",
        "    'g_best_state': None,\r\n",
        "    'd_best_state': None\r\n",
        "}\r\n",
        "ade_list, fde_list, train_ade, train_fde, avg_speed_error, f_speed_error = [], [], [], [], [], []\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQWgomzWByUC"
      },
      "source": [
        "def discriminator_step(batch, generator, discriminator, d_loss_fn, optimizer_d):\r\n",
        "    \"\"\"This step is similar to Social GAN Code\"\"\"\r\n",
        "    if USE_GPU:\r\n",
        "        batch = [tensor.cuda() for tensor in batch]\r\n",
        "    else:\r\n",
        "        batch = [tensor for tensor in batch]\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed, pred_ped_speed, obs_label, pred_label,\r\n",
        "         obs_obj_rel_speed) = batch\r\n",
        "        generator_out, _ = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                  pred_traj_gt, TRAIN_METRIC, None, obs_obj_rel_speed, obs_label=obs_label, pred_label=pred_label)\r\n",
        "    else:\r\n",
        "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed, pred_ped_speed, obs_obj_rel_speed) = batch\r\n",
        "        generator_out, _ = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                  pred_traj_gt, TRAIN_METRIC, None, obs_obj_rel_speed, obs_label=None, pred_label=None)\r\n",
        "\r\n",
        "    losses = {}\r\n",
        "    loss = torch.zeros(1).to(pred_traj_gt)\r\n",
        "\r\n",
        "    pred_traj_fake_rel = generator_out\r\n",
        "    pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\r\n",
        "\r\n",
        "    traj_real = torch.cat([obs_traj, pred_traj_gt], dim=0)\r\n",
        "    traj_real_rel = torch.cat([obs_traj_rel, pred_traj_gt_rel], dim=0)\r\n",
        "    traj_fake = torch.cat([obs_traj, pred_traj_fake], dim=0)\r\n",
        "    traj_fake_rel = torch.cat([obs_traj_rel, pred_traj_fake_rel], dim=0)\r\n",
        "    ped_speed = torch.cat([obs_ped_speed, pred_ped_speed], dim=0)\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        label_info = torch.cat([obs_label, pred_label], dim=0)\r\n",
        "        scores_fake = discriminator(traj_fake, traj_fake_rel, ped_speed, label=label_info)\r\n",
        "        scores_real = discriminator(traj_real, traj_real_rel, ped_speed, label=label_info)\r\n",
        "    else:\r\n",
        "        scores_fake = discriminator(traj_fake, traj_fake_rel, ped_speed, label=None)\r\n",
        "        scores_real = discriminator(traj_real, traj_real_rel, ped_speed, label=None)\r\n",
        "\r\n",
        "    data_loss = d_loss_fn(scores_real, scores_fake)\r\n",
        "    losses['D_data_loss'] = data_loss.item()\r\n",
        "    loss += data_loss\r\n",
        "    losses['D_total_loss'] = loss.item()\r\n",
        "\r\n",
        "    optimizer_d.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer_d.step()\r\n",
        "\r\n",
        "    return losses\r\n",
        "\r\n",
        "\r\n",
        "def speed_regressor_step(batch, generator, speed_regressor, optimizer_speed_regressor):\r\n",
        "    losses = {}\r\n",
        "    speed_loss = []\r\n",
        "\r\n",
        "    if USE_GPU:\r\n",
        "        batch = [tensor.cuda() for tensor in batch]\r\n",
        "    else:\r\n",
        "        batch = [tensor for tensor in batch]\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "        obs_label, pred_label, obs_obj_rel_speed) = batch\r\n",
        "    else:\r\n",
        "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed,\r\n",
        "         pred_ped_speed, obs_obj_rel_speed) = batch\r\n",
        "\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        _, final_enc_h = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                              pred_traj_gt, TRAIN_METRIC, None, obs_obj_rel_speed, obs_label=obs_label, pred_label=pred_label)\r\n",
        "    else:\r\n",
        "        _, final_enc_h = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                  pred_traj_gt, TRAIN_METRIC, None, obs_obj_rel_speed, obs_label=None, pred_label=None)\r\n",
        "\r\n",
        "    fake_ped_speed = speed_regressor(obs_ped_speed, final_enc_h)\r\n",
        "    loss_mask = loss_mask[:, OBS_LEN:]\r\n",
        "\r\n",
        "    speed_loss.append(L2_LOSS_WEIGHT * mae_loss(\r\n",
        "            fake_ped_speed,\r\n",
        "            pred_ped_speed,\r\n",
        "            mode='raw',\r\n",
        "            speed_reg='speed_regressor'))\r\n",
        "\r\n",
        "    total_speed_loss = torch.zeros(1).to(pred_ped_speed)\r\n",
        "    speed_loss = torch.stack(speed_loss, dim=1)\r\n",
        "\r\n",
        "    for start, end in seq_start_end.data:\r\n",
        "        _speed_loss = speed_loss[start:end]\r\n",
        "        _speed_loss = torch.sum(_speed_loss, dim=0)\r\n",
        "        _speed_loss = torch.min(_speed_loss) / torch.sum(loss_mask[start:end])\r\n",
        "        total_speed_loss += _speed_loss\r\n",
        "    losses['Speed_Regression_Loss'] = total_speed_loss.item()\r\n",
        "\r\n",
        "    optimizer_speed_regressor.zero_grad()\r\n",
        "    total_speed_loss.backward()\r\n",
        "    optimizer_speed_regressor.step()\r\n",
        "\r\n",
        "    return losses\r\n",
        "\r\n",
        "\r\n",
        "def generator_step(batch, generator, discriminator, g_loss_fn, optimizer_g):\r\n",
        "    \"\"\"This step is similar to Social GAN Code\"\"\"\r\n",
        "    if USE_GPU:\r\n",
        "        batch = [tensor.cuda() for tensor in batch]\r\n",
        "    else:\r\n",
        "        batch = [tensor for tensor in batch]\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "        obs_label, pred_label, obs_obj_rel_speed) = batch\r\n",
        "    else:\r\n",
        "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed, pred_ped_speed, obs_obj_rel_speed) = batch\r\n",
        "\r\n",
        "    losses = {}\r\n",
        "    loss = torch.zeros(1).to(pred_traj_gt)\r\n",
        "    g_l2_loss_rel = []\r\n",
        "\r\n",
        "    loss_mask = loss_mask[:, OBS_LEN:]\r\n",
        "\r\n",
        "    for _ in range(BEST_K):\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            generator_out, final_enc_h = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                  pred_traj_gt, TRAIN_METRIC, None, obs_obj_rel_speed, obs_label=obs_label, pred_label=pred_label)\r\n",
        "        else:\r\n",
        "            generator_out, final_enc_h = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                      pred_traj_gt, TRAIN_METRIC, None, obs_obj_rel_speed, obs_label=None, pred_label=None)\r\n",
        "\r\n",
        "        pred_traj_fake_rel = generator_out\r\n",
        "        pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\r\n",
        "\r\n",
        "        if L2_LOSS_WEIGHT > 0:\r\n",
        "            g_l2_loss_rel.append(L2_LOSS_WEIGHT * l2_loss(\r\n",
        "                pred_traj_fake_rel,\r\n",
        "                pred_traj_gt_rel,\r\n",
        "                loss_mask,\r\n",
        "                mode='raw'))\r\n",
        "\r\n",
        "    g_l2_loss_sum_rel = torch.zeros(1).to(pred_traj_gt)\r\n",
        "    if L2_LOSS_WEIGHT > 0:\r\n",
        "        g_l2_loss_rel = torch.stack(g_l2_loss_rel, dim=1)\r\n",
        "        for start, end in seq_start_end.data:\r\n",
        "            _g_l2_loss_rel = g_l2_loss_rel[start:end]\r\n",
        "            _g_l2_loss_rel = torch.sum(_g_l2_loss_rel, dim=0)\r\n",
        "            _g_l2_loss_rel = torch.min(_g_l2_loss_rel) / torch.sum(loss_mask[start:end])\r\n",
        "            g_l2_loss_sum_rel += _g_l2_loss_rel\r\n",
        "        losses['G_l2_loss_rel'] = g_l2_loss_sum_rel.item()\r\n",
        "        loss += g_l2_loss_sum_rel\r\n",
        "    traj_fake = torch.cat([obs_traj, pred_traj_fake], dim=0)\r\n",
        "    traj_fake_rel = torch.cat([obs_traj_rel, pred_traj_fake_rel], dim=0)\r\n",
        "    ped_speed = torch.cat([obs_ped_speed, pred_ped_speed], dim=0)\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        label_info = torch.cat([obs_label, pred_label], dim=0)\r\n",
        "        scores_fake = discriminator(traj_fake, traj_fake_rel, ped_speed, label=label_info)\r\n",
        "    else:\r\n",
        "        scores_fake = discriminator(traj_fake, traj_fake_rel, ped_speed, label=None)\r\n",
        "    discriminator_loss = g_loss_fn(scores_fake)\r\n",
        "\r\n",
        "    loss += discriminator_loss\r\n",
        "    losses['G_discriminator_loss'] = discriminator_loss.item()\r\n",
        "    losses['G_total_loss'] = loss.item()\r\n",
        "\r\n",
        "    optimizer_g.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer_g.step()\r\n",
        "\r\n",
        "    return losses\r\n",
        "\r\n",
        "\r\n",
        "def check_accuracy(loader, generator, discriminator, d_loss_fn, speed_regressor):\r\n",
        "    d_losses = []\r\n",
        "    metrics = {}\r\n",
        "    g_l2_losses_abs, g_l2_losses_rel = ([],) * 2\r\n",
        "    disp_error, f_disp_error, mean_speed_disp_error, final_speed_disp_error = [], [], [], []\r\n",
        "    total_traj = 0\r\n",
        "    loss_mask_sum = 0\r\n",
        "    generator.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch in loader:\r\n",
        "            if USE_GPU:\r\n",
        "                batch = [tensor.cuda() for tensor in batch]\r\n",
        "            else:\r\n",
        "                batch = [tensor for tensor in batch]\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed,\r\n",
        "                 pred_ped_speed, obs_label, pred_label, obs_obj_rel_speed) = batch\r\n",
        "            else:\r\n",
        "                (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed,\r\n",
        "                 pred_ped_speed, obs_obj_rel_speed) = batch\r\n",
        "\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                pred_traj_fake_rel, final_enc_h = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                  pred_traj_gt, TRAIN_METRIC, None, obs_obj_rel_speed, obs_label=obs_label, pred_label=pred_label)\r\n",
        "            else:\r\n",
        "                pred_traj_fake_rel, final_enc_h = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                      pred_traj_gt, TRAIN_METRIC, None, obs_obj_rel_speed, obs_label=None, pred_label=None)\r\n",
        "\r\n",
        "            fake_ped_speed = speed_regressor(obs_ped_speed, final_enc_h)\r\n",
        "\r\n",
        "            pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\r\n",
        "            loss_mask = loss_mask[:, OBS_LEN:]\r\n",
        "\r\n",
        "            g_l2_loss_abs, g_l2_loss_rel = cal_l2_losses(\r\n",
        "                pred_traj_gt, pred_traj_gt_rel, pred_traj_fake,\r\n",
        "                pred_traj_fake_rel, loss_mask\r\n",
        "            )\r\n",
        "\r\n",
        "            abs_speed_los = cal_mae_speed_loss(pred_ped_speed, fake_ped_speed)\r\n",
        "            ade = displacement_error(pred_traj_gt, pred_traj_fake)\r\n",
        "            fde = final_displacement_error(pred_traj_gt, pred_traj_fake)\r\n",
        "\r\n",
        "            traj_real = torch.cat([obs_traj, pred_traj_gt], dim=0)\r\n",
        "            traj_real_rel = torch.cat([obs_traj_rel, pred_traj_gt_rel], dim=0)\r\n",
        "            traj_fake = torch.cat([obs_traj, pred_traj_fake], dim=0)\r\n",
        "            traj_fake_rel = torch.cat([obs_traj_rel, pred_traj_fake_rel], dim=0)\r\n",
        "            ped_speed = torch.cat([obs_ped_speed, pred_ped_speed], dim=0)\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                label_info = torch.cat([obs_label, pred_label], dim=0)\r\n",
        "                scores_fake = discriminator(traj_fake, traj_fake_rel, ped_speed, label=label_info)\r\n",
        "                scores_real = discriminator(traj_real, traj_real_rel, ped_speed, label=label_info)\r\n",
        "            else:\r\n",
        "                scores_fake = discriminator(traj_fake, traj_fake_rel, ped_speed, label=None)\r\n",
        "                scores_real = discriminator(traj_real, traj_real_rel, ped_speed, label=None)\r\n",
        "\r\n",
        "            d_loss = d_loss_fn(scores_real, scores_fake)\r\n",
        "            d_losses.append(d_loss.item())\r\n",
        "\r\n",
        "            g_l2_losses_abs.append(g_l2_loss_abs.item())\r\n",
        "            g_l2_losses_rel.append(g_l2_loss_rel.item())\r\n",
        "            disp_error.append(ade.item())\r\n",
        "            f_disp_error.append(fde.item())\r\n",
        "            mean_speed_disp_error.append(abs_speed_los.item())\r\n",
        "\r\n",
        "            loss_mask_sum += torch.numel(loss_mask.data)\r\n",
        "            total_traj += pred_traj_gt.size(1)\r\n",
        "            if total_traj >= NUM_SAMPLE_CHECK:\r\n",
        "                break\r\n",
        "\r\n",
        "    metrics['d_loss'] = sum(d_losses) / len(d_losses)\r\n",
        "    metrics['g_l2_loss_abs'] = sum(g_l2_losses_abs) / loss_mask_sum\r\n",
        "    metrics['g_l2_loss_rel'] = sum(g_l2_losses_rel) / loss_mask_sum\r\n",
        "    metrics['ade'] = sum(disp_error) / (total_traj * PRED_LEN)\r\n",
        "    metrics['fde'] = sum(f_disp_error) / total_traj\r\n",
        "    metrics['mean_l2_speed'] = sum(mean_speed_disp_error) / len(mean_speed_disp_error)\r\n",
        "\r\n",
        "    generator.train()\r\n",
        "    return metrics\r\n",
        "\r\n",
        "\r\n",
        "def cal_l2_losses(pred_traj_gt, pred_traj_gt_rel, pred_traj_fake, pred_traj_fake_rel, loss_mask):\r\n",
        "    g_l2_loss_abs = l2_loss(pred_traj_fake, pred_traj_gt, loss_mask, mode='sum')\r\n",
        "    g_l2_loss_rel = l2_loss(pred_traj_fake_rel, pred_traj_gt_rel, loss_mask, mode='sum')\r\n",
        "    return g_l2_loss_abs, g_l2_loss_rel\r\n",
        "\r\n",
        "\r\n",
        "def cal_mae_speed_loss(pred_speed_gt, pred_speed_fake):\r\n",
        "    g_l2_speed_loss = mae_loss(pred_speed_gt, pred_speed_fake, speed_reg='speed_reg', mode='sum')\r\n",
        "    return g_l2_speed_loss\r\n",
        "\r\n",
        "\r\n",
        "def cal_msae(real_speed, fake_traj):\r\n",
        "    fake_output_speed = fake_speed(fake_traj)\r\n",
        "    real_speed = real_speed.permute(1, 0, 2)\r\n",
        "    msae = mean_speed_error(real_speed, fake_output_speed)\r\n",
        "    return msae\r\n",
        "\r\n",
        "\r\n",
        "def fake_speed(fake_traj):\r\n",
        "    output_speed = []\r\n",
        "    sigmoid_speed = nn.Sigmoid()\r\n",
        "    for a, b in zip(fake_traj[:, :], fake_traj[1:, :]):\r\n",
        "        dist = torch.pairwise_distance(a, b)\r\n",
        "        speed = sigmoid_speed(dist)\r\n",
        "        output_speed.append(speed.view(1, -1))\r\n",
        "    output_fake_speed = torch.cat(output_speed, dim=0).unsqueeze(dim=2).permute(1, 0, 2)\r\n",
        "    return output_fake_speed\r\n",
        "\r\n",
        "\r\n",
        "def cal_fse(real_speed, fake_traj):\r\n",
        "    last_two_traj_info = fake_traj[-2:, :, :]\r\n",
        "    fake_output_speed = fake_speed(last_two_traj_info)\r\n",
        "    fse = final_speed_error(real_speed.unsqueeze(dim=2), fake_output_speed)\r\n",
        "    return fse\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b88Je4r3ErWp"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqtOLCkFByXx",
        "outputId": "125d8fe4-bfcc-4559-802d-ce0b1f107812"
      },
      "source": [
        "val_ade_list, val_fde_list, train_ade, train_fde, train_avg_speed_error, val_avg_speed_error, val_msae_list = [], [], [], [], [], [], []\r\n",
        "train_ade_list, train_fde_list = [], []\r\n",
        "while epoch < required_epoch:\r\n",
        "    gc.collect()\r\n",
        "    d_steps_left, g_steps_left, speed_regression_steps_left = D_STEPS, G_STEPS, SR_STEPS\r\n",
        "    epoch += 1\r\n",
        "    print('Starting epoch {}'.format(epoch))\r\n",
        "    disc_loss, gent_loss, sr_loss = [], [], []\r\n",
        "    for batch in train_loader:\r\n",
        "        if d_steps_left > 0:\r\n",
        "            losses_d = discriminator_step(batch, generator, discriminator, d_loss_fn, optimizer_d)\r\n",
        "            disc_loss.append(losses_d['D_total_loss'])\r\n",
        "            d_steps_left -= 1\r\n",
        "        elif g_steps_left > 0:\r\n",
        "            losses_g = generator_step(batch, generator, discriminator, g_loss_fn, optimizer_g)\r\n",
        "            speed_regression_loss = speed_regressor_step(batch, generator, speed_regressor, optimizer_speed_regressor)\r\n",
        "            losses_g['Speed_Regression_Loss'] = speed_regression_loss['Speed_Regression_Loss']\r\n",
        "            sr_loss.append(speed_regression_loss['Speed_Regression_Loss'])\r\n",
        "            gent_loss.append(losses_g['G_discriminator_loss'])\r\n",
        "            g_steps_left -= 1\r\n",
        "\r\n",
        "        if d_steps_left > 0 or g_steps_left > 0:\r\n",
        "            continue\r\n",
        "\r\n",
        "        #gen_writer.add_scalar('loss', gent_loss[0], epoch)\r\n",
        "        #dis_writer.add_scalar('loss', disc_loss[0], epoch)\r\n",
        "        #gen_writer.close()\r\n",
        "        #dis_writer.close()\r\n",
        "        #disc_loss.clear()\r\n",
        "        #gent_loss.clear()\r\n",
        "\r\n",
        "        if t > 0 and t % CHECKPOINT_EVERY == 0:\r\n",
        "\r\n",
        "            print('t = {} / {}'.format(t + 1, NUM_ITERATIONS))\r\n",
        "            for k, v in sorted(losses_d.items()):\r\n",
        "                print('  [D] {}: {:.3f}'.format(k, v))\r\n",
        "            for k, v in sorted(losses_g.items()):\r\n",
        "                print('  [G] {}: {:.3f}'.format(k, v))\r\n",
        "\r\n",
        "            print('Checking stats on val ...')\r\n",
        "            metrics_val = check_accuracy(val_loader, generator, discriminator, d_loss_fn, speed_regressor)\r\n",
        "            print('Checking stats on train ...')\r\n",
        "            metrics_train = check_accuracy(train_loader, generator, discriminator, d_loss_fn, speed_regressor)\r\n",
        "\r\n",
        "            for k, v in sorted(metrics_val.items()):\r\n",
        "                print('  [val] {}: {:.3f}'.format(k, v))\r\n",
        "            for k, v in sorted(metrics_train.items()):\r\n",
        "                print('  [train] {}: {:.3f}'.format(k, v))\r\n",
        "\r\n",
        "            val_ade_list.append(metrics_val['ade'])\r\n",
        "            val_fde_list.append(metrics_val['fde'])\r\n",
        "\r\n",
        "            train_ade_list.append(metrics_train['ade'])\r\n",
        "            train_fde_list.append(metrics_train['fde'])\r\n",
        "            val_msae_list.append(metrics_val['mean_l2_speed'])\r\n",
        "\r\n",
        "            if metrics_val.get('ade') == min(val_ade_list) or metrics_val['ade'] < min(val_ade_list) or metrics_val.get('fde') == min(val_fde_list) or metrics_val['fde'] < min(val_fde_list):\r\n",
        "                checkpoint['g_best_state'] = generator.state_dict()\r\n",
        "            if metrics_val.get('ade') == min(val_ade_list) or metrics_val['ade'] < min(val_ade_list):\r\n",
        "                print('New low for avg_disp_error')\r\n",
        "                checkpoint['best_g_state'] = generator.state_dict()\r\n",
        "            if metrics_val.get('fde') == min(val_fde_list) or metrics_val['fde'] < min(val_fde_list):\r\n",
        "                print('New low for final_disp_error')\r\n",
        "            if metrics_val.get('mean_l2_speed') == min(val_msae_list) or metrics_val['mean_l2_speed'] < min(val_msae_list):\r\n",
        "                print('New low for Speed regressor model')\r\n",
        "                checkpoint['best_regressor_state'] = speed_regressor.state_dict()\r\n",
        "\r\n",
        "            checkpoint['g_state'] = generator.state_dict()\r\n",
        "            checkpoint['g_optim_state'] = optimizer_g.state_dict()\r\n",
        "            checkpoint['d_state'] = discriminator.state_dict()\r\n",
        "            checkpoint['d_optim_state'] = optimizer_d.state_dict()\r\n",
        "            checkpoint['regressor_state'] = speed_regressor.state_dict()\r\n",
        "            torch.save(checkpoint, CHECKPOINT_NAME)\r\n",
        "            print('Done.')\r\n",
        "\r\n",
        "        t += 1\r\n",
        "        d_steps_left = D_STEPS\r\n",
        "        g_steps_left = G_STEPS\r\n",
        "        if t >= NUM_ITERATIONS:\r\n",
        "            break\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Starting epoch 2\n",
            "Starting epoch 3\n",
            "t = 101 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.072\n",
            "  [G] G_total_loss: 0.765\n",
            "  [G] Speed_Regression_Loss: 0.385\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.820\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 1.710\n",
            "  [val] g_l2_loss_abs: 1.312\n",
            "  [val] g_l2_loss_rel: 1.312\n",
            "  [val] mean_l2_speed: 20.275\n",
            "  [train] ade: 0.883\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 1.215\n",
            "  [train] g_l2_loss_abs: 1.212\n",
            "  [train] g_l2_loss_rel: 1.212\n",
            "  [train] mean_l2_speed: 76.526\n",
            "New low for avg_disp_error\n",
            "New low for final_disp_error\n",
            "New low for Speed regressor model\n",
            "Done.\n",
            "Starting epoch 4\n",
            "Starting epoch 5\n",
            "Starting epoch 6\n",
            "t = 201 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.095\n",
            "  [G] G_total_loss: 0.788\n",
            "  [G] Speed_Regression_Loss: 0.480\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.637\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 1.356\n",
            "  [val] g_l2_loss_abs: 0.700\n",
            "  [val] g_l2_loss_rel: 0.700\n",
            "  [val] mean_l2_speed: 25.559\n",
            "  [train] ade: 0.579\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 1.205\n",
            "  [train] g_l2_loss_abs: 0.713\n",
            "  [train] g_l2_loss_rel: 0.713\n",
            "  [train] mean_l2_speed: 38.627\n",
            "New low for avg_disp_error\n",
            "New low for final_disp_error\n",
            "Done.\n",
            "Starting epoch 7\n",
            "Starting epoch 8\n",
            "Starting epoch 9\n",
            "t = 301 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.059\n",
            "  [G] G_total_loss: 0.752\n",
            "  [G] Speed_Regression_Loss: 0.319\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.538\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 1.212\n",
            "  [val] g_l2_loss_abs: 0.585\n",
            "  [val] g_l2_loss_rel: 0.585\n",
            "  [val] mean_l2_speed: 20.968\n",
            "  [train] ade: 0.513\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 0.962\n",
            "  [train] g_l2_loss_abs: 0.540\n",
            "  [train] g_l2_loss_rel: 0.540\n",
            "  [train] mean_l2_speed: 36.237\n",
            "New low for avg_disp_error\n",
            "New low for final_disp_error\n",
            "Done.\n",
            "Starting epoch 10\n",
            "Starting epoch 11\n",
            "t = 401 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.046\n",
            "  [G] G_total_loss: 0.739\n",
            "  [G] Speed_Regression_Loss: 0.239\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.331\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 0.755\n",
            "  [val] g_l2_loss_abs: 0.229\n",
            "  [val] g_l2_loss_rel: 0.229\n",
            "  [val] mean_l2_speed: 16.940\n",
            "  [train] ade: 0.367\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 0.807\n",
            "  [train] g_l2_loss_abs: 0.314\n",
            "  [train] g_l2_loss_rel: 0.314\n",
            "  [train] mean_l2_speed: 21.002\n",
            "New low for avg_disp_error\n",
            "New low for final_disp_error\n",
            "New low for Speed regressor model\n",
            "Done.\n",
            "Starting epoch 12\n",
            "Starting epoch 13\n",
            "Starting epoch 14\n",
            "t = 501 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.062\n",
            "  [G] G_total_loss: 0.755\n",
            "  [G] Speed_Regression_Loss: 0.301\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.435\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 1.237\n",
            "  [val] g_l2_loss_abs: 0.470\n",
            "  [val] g_l2_loss_rel: 0.470\n",
            "  [val] mean_l2_speed: 13.605\n",
            "  [train] ade: 0.334\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 0.725\n",
            "  [train] g_l2_loss_abs: 0.244\n",
            "  [train] g_l2_loss_rel: 0.244\n",
            "  [train] mean_l2_speed: 21.982\n",
            "New low for Speed regressor model\n",
            "Done.\n",
            "Starting epoch 15\n",
            "Starting epoch 16\n",
            "Starting epoch 17\n",
            "t = 601 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.064\n",
            "  [G] G_total_loss: 0.757\n",
            "  [G] Speed_Regression_Loss: 0.288\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.396\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 0.993\n",
            "  [val] g_l2_loss_abs: 0.334\n",
            "  [val] g_l2_loss_rel: 0.334\n",
            "  [val] mean_l2_speed: 15.488\n",
            "  [train] ade: 0.499\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 1.040\n",
            "  [train] g_l2_loss_abs: 0.557\n",
            "  [train] g_l2_loss_rel: 0.557\n",
            "  [train] mean_l2_speed: 20.631\n",
            "Done.\n",
            "Starting epoch 18\n",
            "Starting epoch 19\n",
            "t = 701 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.056\n",
            "  [G] G_total_loss: 0.750\n",
            "  [G] Speed_Regression_Loss: 0.280\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.281\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 0.715\n",
            "  [val] g_l2_loss_abs: 0.191\n",
            "  [val] g_l2_loss_rel: 0.191\n",
            "  [val] mean_l2_speed: 10.700\n",
            "  [train] ade: 0.386\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 0.886\n",
            "  [train] g_l2_loss_abs: 0.275\n",
            "  [train] g_l2_loss_rel: 0.275\n",
            "  [train] mean_l2_speed: 16.910\n",
            "New low for avg_disp_error\n",
            "New low for final_disp_error\n",
            "New low for Speed regressor model\n",
            "Done.\n",
            "Starting epoch 20\n",
            "Starting epoch 21\n",
            "Starting epoch 22\n",
            "t = 801 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.062\n",
            "  [G] G_total_loss: 0.755\n",
            "  [G] Speed_Regression_Loss: 0.302\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.275\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 0.714\n",
            "  [val] g_l2_loss_abs: 0.154\n",
            "  [val] g_l2_loss_rel: 0.154\n",
            "  [val] mean_l2_speed: 11.909\n",
            "  [train] ade: 0.355\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 0.689\n",
            "  [train] g_l2_loss_abs: 0.252\n",
            "  [train] g_l2_loss_rel: 0.252\n",
            "  [train] mean_l2_speed: 21.345\n",
            "New low for avg_disp_error\n",
            "New low for final_disp_error\n",
            "Done.\n",
            "Starting epoch 23\n",
            "Starting epoch 24\n",
            "Starting epoch 25\n",
            "t = 901 / 1403\n",
            "  [D] D_data_loss: 1.386\n",
            "  [D] D_total_loss: 1.386\n",
            "  [G] G_discriminator_loss: 0.693\n",
            "  [G] G_l2_loss_rel: 0.035\n",
            "  [G] G_total_loss: 0.728\n",
            "  [G] Speed_Regression_Loss: 0.223\n",
            "Checking stats on val ...\n",
            "Checking stats on train ...\n",
            "  [val] ade: 0.264\n",
            "  [val] d_loss: 1.386\n",
            "  [val] fde: 0.671\n",
            "  [val] g_l2_loss_abs: 0.178\n",
            "  [val] g_l2_loss_rel: 0.178\n",
            "  [val] mean_l2_speed: 18.033\n",
            "  [train] ade: 0.292\n",
            "  [train] d_loss: 1.386\n",
            "  [train] fde: 0.615\n",
            "  [train] g_l2_loss_abs: 0.201\n",
            "  [train] g_l2_loss_rel: 0.201\n",
            "  [train] mean_l2_speed: 28.130\n",
            "New low for avg_disp_error\n",
            "New low for final_disp_error\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw0sIAfXXRPx"
      },
      "source": [
        "\r\n",
        "def get_traj(trajectories, sequences, labels=None):\r\n",
        "    print(\"Enter the sequence you want to visualize from:\", sequences)\r\n",
        "    seq_start = int(input(\"Enter the sequence start: \"))\r\n",
        "    seq_end = int(input(\"Enter the sequence end:\"))\r\n",
        "    positions = trajectories[:, seq_start:seq_end, :]\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        label = labels[:, seq_start:seq_end, :]\r\n",
        "        return positions, label\r\n",
        "    else:\r\n",
        "        return positions\r\n",
        "\r\n",
        "\r\n",
        "def get_distance(trajectories):\r\n",
        "    euclid_distance = []\r\n",
        "    for a, b in zip(trajectories[:, :], trajectories[1:, :]):\r\n",
        "        dist = torch.pairwise_distance(a, b)\r\n",
        "        dist = dist.cpu().detach().numpy()\r\n",
        "        euclid_distance.append(dist.reshape(1, -1))\r\n",
        "    euclid_distance = torch.from_numpy(np.concatenate(euclid_distance, axis=0)).type(torch.float)\r\n",
        "    return euclid_distance\r\n",
        "\r\n",
        "\r\n",
        "def inverse_sigmoid(speeds, max_speed=None, labels=None):\r\n",
        "    simulated_speed = []\r\n",
        "    inv = torch.log((speeds / (1 - speeds)))\r\n",
        "    if SINGLE_CONDITIONAL_MODEL:\r\n",
        "        print(\"The current speeds are: \", inv / max_speed)\r\n",
        "    else:\r\n",
        "        labels = labels.view(PRED_LEN, -1)\r\n",
        "        for speed, agent in zip(inv, labels[:PRED_LEN-1, :]):\r\n",
        "            for a, b, in zip(speed, agent):\r\n",
        "                if torch.eq(b, 0.1):\r\n",
        "                    s = a / AV_MAX_SPEED\r\n",
        "                    simulated_speed.append(s.view(1, 1))\r\n",
        "                elif torch.eq(b, 0.2):\r\n",
        "                    s = a / OTHER_MAX_SPEED\r\n",
        "                    simulated_speed.append(s.view(1, 1))\r\n",
        "                elif torch.eq(b, 0.3):\r\n",
        "                    s = a / AGENT_MAX_SPEED\r\n",
        "                    simulated_speed.append(s.view(1, 1))\r\n",
        "        simulated_speed = torch.cat(simulated_speed, dim=0)\r\n",
        "        print('the labels are: ', labels)\r\n",
        "        print(\"The current speeds are: \", simulated_speed.view(PRED_LEN-1, -1))\r\n",
        "\r\n",
        "\r\n",
        "def get_speed_from_distance(distance):\r\n",
        "    # Since we skip the speed calculation (see trajectories.py for more explanation), we directly pass the distance through sigmoid layer\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        sigmoid_speed = torch.sigmoid(distance)\r\n",
        "    else:\r\n",
        "        speed = distance / FRAMES_PER_SECOND_SINGLE_CONDITION\r\n",
        "        sigmoid_speed = torch.sigmoid(speed)\r\n",
        "    return sigmoid_speed\r\n",
        "\r\n",
        "\r\n",
        "def get_max_speed(path):\r\n",
        "    if path == \"eth\":\r\n",
        "        return ZARA1_MAX_SPEED\r\n",
        "    elif path == \"hotel\":\r\n",
        "        return ETH_MAX_SPEED\r\n",
        "    elif path == \"zara1\":\r\n",
        "        return ETH_MAX_SPEED\r\n",
        "    elif path == \"zara2\":\r\n",
        "        return ETH_MAX_SPEED\r\n",
        "    elif path == \"univ\":\r\n",
        "        return ETH_MAX_SPEED\r\n",
        "\r\n",
        "\r\n",
        "def verify_speed(traj, sequences, labels=None):\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        traj, label = get_traj(traj, sequences, labels=labels)\r\n",
        "    else:\r\n",
        "        dataset_name = get_dataset_name(SINGLE_TEST_DATASET_PATH)\r\n",
        "        traj = get_traj(traj, sequences, labels=None)\r\n",
        "    dist = get_distance(traj)\r\n",
        "    speed = get_speed_from_distance(dist)\r\n",
        "    # We calculate inverse sigmoid to verify the speed\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        inverse_sigmoid(speed, labels=label)\r\n",
        "    else:\r\n",
        "        maxspeed= get_max_speed(dataset_name)\r\n",
        "        inverse_sigmoid(speed, max_speed=maxspeed)\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "TUzZbRC0Fa5k",
        "outputId": "06422e03-0c05-45e5-efc7-4892ea8d9c3b"
      },
      "source": [
        "hi"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-55ca6286e3e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'hi' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvQ1_0NlBya0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "1e878aa9-670f-4c22-dc30-0554f120124d"
      },
      "source": [
        "\r\n",
        "from scipy.spatial.distance import pdist, squareform\r\n",
        "\r\n",
        "\r\n",
        "def collisionPercentage(traj, sequences):\r\n",
        "    collided_or_not = []\r\n",
        "    no_of_frames = 0\r\n",
        "    for (start, end) in sequences:\r\n",
        "        curr_Traj = traj[:, start:end, :]\r\n",
        "        curr_Traj = traj[:, start:end, :].cpu().data.numpy()\r\n",
        "        # no_of_frames += curr_frame\r\n",
        "        curr_collided_peds = 0\r\n",
        "        peds = 0\r\n",
        "        for trajectories in curr_Traj:\r\n",
        "            peds += trajectories.shape[0]\r\n",
        "            dist = squareform(pdist(trajectories, metric=\"euclidean\"))\r\n",
        "            np.fill_diagonal(dist, np.nan)\r\n",
        "            for rows in dist:\r\n",
        "                if any(i <= 0.1 for i in rows):\r\n",
        "                    curr_collided_peds += 1\r\n",
        "\r\n",
        "        percentage_of_collision_in_curr_frame = curr_collided_peds / peds\r\n",
        "        collided_or_not.append(percentage_of_collision_in_curr_frame)\r\n",
        "\r\n",
        "    collision = sum(collided_or_not) / len(collided_or_not)\r\n",
        "    a = sum(collided_or_not)\r\n",
        "\r\n",
        "    return torch.tensor(collision)\r\n",
        "\r\n",
        "\r\n",
        "def evaluate_helper(error, traj, seq_start_end):\r\n",
        "    sum_ = []\r\n",
        "    curr_best_traj = []\r\n",
        "    for (start, end) in seq_start_end:\r\n",
        "        sum_.append(torch.min(torch.sum(error[start.item():end.item()], dim=0)))\r\n",
        "        idx = torch.argmin(torch.sum(error[start.item():end.item()], dim=0))\r\n",
        "        curr_best_traj.append(traj[idx, :, start:end, :])\r\n",
        "    return torch.cat(curr_best_traj, dim=1), sum(sum_)\r\n",
        "\r\n",
        "\r\n",
        "def evaluate(loader, generator, num_samples, speed_regressor):\r\n",
        "    ade_outer, fde_outer, simulated_output, total_traj, sequences, labels, observed_traj = [], [], [], [], [], [], []\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch in loader:\r\n",
        "            if USE_GPU:\r\n",
        "                batch = [tensor.cuda() for tensor in batch]\r\n",
        "            else:\r\n",
        "                batch = [tensor for tensor in batch]\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed,\r\n",
        "                 pred_ped_speed, obs_label, pred_label, obs_obj_rel_speed) = batch\r\n",
        "            else:\r\n",
        "                (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, loss_mask, seq_start_end, obs_ped_speed,\r\n",
        "                 pred_ped_speed, obs_obj_rel_speed) = batch\r\n",
        "\r\n",
        "            ade, fde, traj_op, traj_obs = [], [], [], []\r\n",
        "            total_traj.append(pred_traj_gt.size(1))\r\n",
        "            sequences.append(seq_start_end)\r\n",
        "            if MULTI_CONDITIONAL_MODEL:\r\n",
        "                labels.append(pred_label)\r\n",
        "\r\n",
        "            for _ in range(num_samples):\r\n",
        "                if MULTI_CONDITIONAL_MODEL:\r\n",
        "                    #fake_pred_speed = speed_regressor()\r\n",
        "                    _, final_enc_h = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                                   pred_traj_gt, 2, None, obs_obj_rel_speed, obs_label=obs_label, pred_label=pred_label)\r\n",
        "                    fake_speed = speed_regressor(obs_ped_speed, final_enc_h)\r\n",
        "                    pred_traj_fake_rel, _ = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                                   pred_traj_gt,\r\n",
        "                                                   TEST_METRIC, fake_speed, obs_obj_rel_speed, obs_label=obs_label, pred_label=pred_label)\r\n",
        "                else:\r\n",
        "                    _, final_enc_h = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                                   pred_traj_gt, 0, None, obs_obj_rel_speed, obs_label=None, pred_label=None)\r\n",
        "                    fake_speed = speed_regressor(obs_ped_speed, final_enc_h)\r\n",
        "                    pred_traj_fake_rel, _ = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\r\n",
        "                                                   pred_traj_gt,\r\n",
        "                                                   TEST_METRIC, fake_speed, obs_obj_rel_speed, obs_label=None, pred_label=None)\r\n",
        "\r\n",
        "                    #for a, b in zip(fake_speed, pred_ped_speed):\r\n",
        "                    #    print(a, b)\r\n",
        "\r\n",
        "\r\n",
        "                pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\r\n",
        "                ade.append(displacement_error(pred_traj_fake, pred_traj_gt, mode='raw'))\r\n",
        "                fde.append(final_displacement_error(pred_traj_fake[-1], pred_traj_gt[-1], mode='raw'))\r\n",
        "                traj_op.append(pred_traj_fake.unsqueeze(dim=0))\r\n",
        "                traj_obs.append(obs_traj.unsqueeze(dim=0))\r\n",
        "                #print('obs_traj', obs_traj)\r\n",
        "                #print('pred_traj', pred_traj_fake)\r\n",
        "                #print('pred_traj_GT', pred_traj_gt)\r\n",
        "\r\n",
        "            best_traj, min_ade_error = evaluate_helper(torch.stack(ade, dim=1), torch.cat(traj_op, dim=0),\r\n",
        "                                                       seq_start_end)\r\n",
        "            #print('best', best_traj)\r\n",
        "            staked_obs = torch.cat(traj_obs, dim=0)\r\n",
        "            obs = staked_obs[0]\r\n",
        "            observed_traj.append(obs)\r\n",
        "            _, min_fde_error = evaluate_helper(torch.stack(fde, dim=1), torch.cat(traj_op, dim=0), seq_start_end)\r\n",
        "            ade_outer.append(min_ade_error)\r\n",
        "            fde_outer.append(min_fde_error)\r\n",
        "            simulated_output.append(best_traj)\r\n",
        "\r\n",
        "        ade = sum(ade_outer) / (sum(total_traj) * PRED_LEN)\r\n",
        "        fde = sum(fde_outer) / (sum(total_traj))\r\n",
        "        simulated_traj = torch.cat(simulated_output, dim=1)\r\n",
        "        total_obs = torch.cat(observed_traj, dim=1).permute(1, 0, 2)\r\n",
        "        if MULTI_CONDITIONAL_MODEL:\r\n",
        "            all_labels = torch.cat(labels, dim=1)\r\n",
        "        last_items_in_sequences = []\r\n",
        "        curr_sequences = []\r\n",
        "        i = 0\r\n",
        "        for sequence_list in sequences:\r\n",
        "            last_sequence = sequence_list[-1]\r\n",
        "            if i > 0:\r\n",
        "                last_items_sum = sum(last_items_in_sequences)\r\n",
        "                curr_sequences.append(last_items_sum + sequence_list)\r\n",
        "            last_items_in_sequences.append(last_sequence[1])\r\n",
        "            if i == 0:\r\n",
        "                curr_sequences.append(sequence_list)\r\n",
        "                i += 1\r\n",
        "                continue\r\n",
        "\r\n",
        "        sequences = torch.cat(curr_sequences, dim=0)\r\n",
        "        colpercent = collisionPercentage(simulated_traj, sequences)\r\n",
        "        #create_data(simulated_traj.permute(1, 0, 2), sequences)\r\n",
        "        print(colpercent * 100)\r\n",
        "\r\n",
        "        if TEST_METRIC == 2:\r\n",
        "            if SINGLE_CONDITIONAL_MODEL:\r\n",
        "                # The speed can be verified for different sequences and this method runs for n number of batches.\r\n",
        "                verify_speed(simulated_traj, sequences, labels=None)\r\n",
        "            else:\r\n",
        "                verify_speed(simulated_traj, sequences, labels=all_labels)\r\n",
        "\r\n",
        "        return ade, fde, colpercent * 100\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    checkpoint = torch.load(CHECKPOINT_NAME)\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        generator = TrajectoryGenerator(mlp_dim=MLP_INPUT_DIM_MULTI_CONDITION,\r\n",
        "                                        h_dim=H_DIM_GENERATOR_MULTI_CONDITION)\r\n",
        "        speed_regressor = SpeedEncoderDecoder(h_dim=H_DIM_GENERATOR_MULTI_CONDITION)\r\n",
        "    else:\r\n",
        "        generator = TrajectoryGenerator(mlp_dim=MLP_INPUT_DIM_SINGLE_CONDITION,\r\n",
        "                                        h_dim=H_DIM_GENERATOR_SINGLE_CONDITION)\r\n",
        "        speed_regressor = SpeedEncoderDecoder(h_dim=H_DIM_GENERATOR_SINGLE_CONDITION)\r\n",
        "    generator.load_state_dict(checkpoint['g_state'])\r\n",
        "    speed_regressor.load_state_dict(checkpoint['regressor_state'])\r\n",
        "    if USE_GPU:\r\n",
        "        generator.cuda()\r\n",
        "    generator.train()\r\n",
        "    speed_regressor.cuda()\r\n",
        "    speed_regressor.train()\r\n",
        "\r\n",
        "    if MULTI_CONDITIONAL_MODEL:\r\n",
        "        test_dataset = MULTI_TEST_DATASET_PATH\r\n",
        "    else:\r\n",
        "        test_dataset = SINGLE_TEST_DATASET_PATH\r\n",
        "    print('Initializing Test dataset')\r\n",
        "    _, loader = data_loader(test_dataset, TEST_METRIC, 'test')\r\n",
        "    print('Test dataset preprocessing done')\r\n",
        "    if TEST_METRIC == 2:\r\n",
        "        num_samples = 20\r\n",
        "    else:\r\n",
        "        num_samples = NUM_SAMPLES\r\n",
        "    cm, ade_final, fde_final = [], [], []\r\n",
        "    for _ in range(20):\r\n",
        "        ade, fde, ca = evaluate(loader, generator, NUM_SAMPLES, speed_regressor)\r\n",
        "        cm.append(ca)\r\n",
        "        ade_final.append(ade)\r\n",
        "        fde_final.append(fde)\r\n",
        "        print('Pred Len: {}, ADE: {:.2f}, FDE: {:.2f}'.format(PRED_LEN, ade, fde))\r\n",
        "    print('average collision: ', sum(cm)/len(cm))\r\n",
        "    print('average ade: ', sum(ade_final) / len(ade_final))\r\n",
        "    print('average fde: ', sum(fde_final) / len(fde_final))\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()\r\n",
        "\r\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Test dataset\n",
            "Test dataset preprocessing done\n",
            "tensor(0.2669)\n",
            "Pred Len: 12, ADE: 0.48, FDE: 0.99\n",
            "tensor(0.2457)\n",
            "Pred Len: 12, ADE: 0.48, FDE: 0.98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-cdccaafc7331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-cdccaafc7331>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0made_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfde_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0made\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed_regressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0made_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0made\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-cdccaafc7331>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(loader, generator, num_samples, speed_regressor)\u001b[0m\n\u001b[1;32m     75\u001b[0m                     pred_traj_fake_rel, _ = generator(obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed,\n\u001b[1;32m     76\u001b[0m                                                    \u001b[0mpred_traj_gt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                                    TEST_METRIC, fake_speed, obs_obj_rel_speed, obs_label=None, pred_label=None)\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;31m#for a, b in zip(fake_speed, pred_ped_speed):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4ef54f8590f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs_traj, obs_traj_rel, seq_start_end, obs_ped_speed, pred_ped_speed, pred_traj, train_or_test, fake_ped_speed, obs_obj_rel_speed, obs_label, pred_label, user_noise)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mmlp_decoder_context_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_encoder_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpm_final_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mAGGREGATION_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0magg_final_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregation_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_encoder_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_start_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_or_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_traj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0mmlp_decoder_context_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_encoder_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_final_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mATTENTION_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4ef54f8590f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h_states, seq_start_end, train_or_test, last_pos, label)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0mreq_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mnew_h_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_hidden_ped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0msorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_h_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_h_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                 \u001b[0mrequired_h_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_ped\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mMAX_CONSIDERED_PED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgDb6Kzzb7VE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS5o0WRo80H5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}